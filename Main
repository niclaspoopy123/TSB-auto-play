 -- =================================================================================
--// INSTRUCTION: This script must be placed in StarterPlayer > StarterPlayerScripts
--// to ensure it runs automatically and persists when your character changes.
-- =================================================================================
--// SELF-LEARNING COMBAT FRAMEWORK V35.0 - "PROJECT APEX ULTRA" (Enhanced by GitHub Copilot)
--// USER: niclaspoopy123
--// BUILD DATE: 2025-10-29
--// UPDATE: Double Q-Learning, enhanced deep learning, hybrid learning, ultra-fast reactions, and adaptive meta-learning.
--// CHANGE: Revolutionary improvements in learning speed, decision accuracy, and combat intelligence.
-- =================================================================================
--[[
V35.0 PATCH NOTES (PROJECT APEX ULTRA):
- NEW (DOUBLE Q-LEARNING): Prevents Q-value overestimation for more accurate action selection
- NEW (HYBRID LEARNING): Combines model-based planning with model-free Q-learning
- NEW (OPPONENT MODELING): Learns and predicts opponent behavior patterns
- NEW (META-LEARNING): Dynamic learning rate adaptation based on performance
- NEW (ADVANCED EXPLORATION): UCB and Boltzmann action selection strategies
- NEW (DEEP STATE ENCODING): Multi-dimensional feature extraction with normalization
- IMPROVED (REACTION TIME): Ultra-fast decision pipeline with minimal overhead
- IMPROVED (ADAPTIVE LEARNING): Context-aware learning rates and tactic adaptation
- IMPROVED (FEATURE WEIGHTS): Auto-tuning based on correlation with success
- OPTIMIZED (PREDICTION): Single-pass vectorized calculations for speed

V34.0 PATCH NOTES (PROJECT APEX):
- NEW (EXPERIENCE REPLAY): Implemented replay buffer to learn from past experiences more efficiently
- NEW (EPSILON-GREEDY EXPLORATION): Dynamic exploration strategy with decay for optimal learning
- NEW (ELIGIBILITY TRACES): Better credit assignment for action sequences
- NEW (DEEP FEATURE EXTRACTION): Multi-layer state representation for better decision-making
- NEW (PRIORITIZED REPLAY): Learn more from important experiences
- IMPROVED (REACTION TIME): Removed unnecessary delays and optimized decision pipeline
- IMPROVED (Q-LEARNING): Enhanced temporal difference learning with n-step returns
- IMPROVED (STATE REPRESENTATION): Better feature normalization and encoding
- OPTIMIZED (PERFORMANCE): Streamlined hot paths and reduced redundant calculations

V33.0 PATCH NOTES (PROJECT TITAN):
- NEW (TRUE Q-LEARNING): The learning algorithm has been upgraded from a simple value update to a proper Q-Learning model. The AI now considers the potential future value of its next best move, leading to more sophisticated, long-term strategies.
- IMPROVED (SMARTER TARGETING): The target-scoring algorithm now penalizes targets moving at high velocity, making the AI prioritize engaging enemies it's more likely to hit.
- IMPROVED (ROBUSTNESS): Added comprehensive checks for target validity throughout the combat loop to prevent errors when a target is lost or destroyed unexpectedly.
- IMPROVED (EXPLOITSEEKER TACTIC): The 'ExploitSeeker' tactic now uses a weighted-random selection, promoting more varied and unpredictable exploratory actions to discover glitches.
- REFACTOR (MODULAR DESIGN): The entire script has been reorganized into logical modules (Services, Constants, AI, CoreLogic, etc.) for significantly improved readability and easier maintenance.
--]]

--// ================= SERVICES ================= //
local Players = game:GetService("Players")
local RunService = game:GetService("RunService")
local Workspace = game:GetService("Workspace")
local LogService = game:GetService("LogService")

--// ================= LOCAL PLAYER ================= //
local player = Players.LocalPlayer

--// ================= AI'S PERSISTENT BRAIN ================= //
-- This table stores the AI's long-term memory, including learned values and statistics.
-- It persists across character resets, allowing the AI to learn over multiple sessions.
local AI = {
  State = {}, -- Volatile state, reset on character spawn.
  Score = 0, -- Persistent score.
  IdentifiedCharacter = "Unknown",
  AvailableMoves = {},
  ActionStats = { -- Persistent learning data.
      totalTrials = 0,
      Aggressive = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Defensive = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      BaitAndPunish = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Finisher = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      ExploitSeeker = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
  },
  --// EXPERIENCE REPLAY: Store past experiences for better learning
  ExperienceBuffer = {
      maxSize = 1000,
      experiences = {},
      priorities = {},
      nextIndex = 1,
  },
  --// DEEP FEATURES: Multi-layer feature extraction with auto-tuning
  FeatureWeights = {
      healthDiff = 1.0,
      distanceToTarget = 1.0,
      energyLevel = 1.0,
      velocityMatch = 1.0,
      tacticSuccess = 1.0,
      comboDensity = 1.0,
      threatLevel = 1.0,
      opportunityWindow = 1.0,
  },
  --// OPPONENT MODEL: Track opponent patterns for prediction
  OpponentModel = {
      actionFrequency = { ATTACK = 0, SPECIAL = 0, EVADE = 0, FEINT = 0, REPOSITION = 0 },
      lastActions = {},
      maxHistorySize = 20,
      predictedNextAction = "ATTACK",
      aggressionScore = 0.5,
  },
  --// HYBRID LEARNING: Model-based and model-free combination
  HybridModel = {
      transitionCounts = {}, -- State-action-nextState transitions
      rewardModel = {}, -- Expected rewards per state-action
      useModelBased = true,
      modelConfidence = 0.3,
  },
  --// DOUBLE Q-LEARNING: Two separate Q-value estimators
  QNetworkB = {
      Aggressive = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Defensive = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      BaitAndPunish = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Finisher = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      ExploitSeeker = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
  },
  --// META-LEARNING: Adaptive learning parameters
  MetaLearning = {
      recentRewards = {},
      maxRewardHistory = 30,
      adaptiveLearningRate = 0.4,
      performanceTrend = 0,
  },
  --// ANOMALY: Module for actively reading the game for glitches.
  GlitchScanner = {
      isGlitchDetected = false,
      glitchType = "None",
      lastVelocityMagnitude = 0,
      lastHumanoidState = "None",
      detectionTime = 0,
  },
  Evaluation = {},
  Actions = {},
  Learning = {}
}

--// ================= CORE INITIALIZATION ================= //

-- This function runs when the player's character is added to the game.
-- It sets up the entire combat system, constants, and event connections.
local function initializeCombatSystem(character)
  --// --- CHARACTER VALIDATION ---
  if not character then return end
  local humanoid = character:WaitForChild("Humanoid")
  local rootPart = character:WaitForChild("HumanoidRootPart")
  local communicateEvent = character:WaitForChild("Communicate", 5)

  if not (humanoid and rootPart and communicateEvent) then
      warn("Project Titan: Character is missing required components (Humanoid, HumanoidRootPart, or Communicate event). AI will not function.")
      return
  end

  --// --- LOCAL STATE & CONNECTIONS ---
  local connections = {} -- Stores all event connections for easy cleanup.
  local targetDiedConnection

  --// --- CHARACTER & MOVE IDENTIFICATION ---
  local CHARACTER_DATA = {
      ["The Slugger"] = { Moves = {"Homerun", "Beatdown", "Grand Slam"}, AttackRange = 35, KiteRange = 45 },
      ["Garou"] = { Moves = {"Whirlwind Kick", "Machine Gun Blows", "Flowing Water"}, AttackRange = 30, KiteRange = 40 },
      ["Genos"] = { Moves = {"Ignition Burst", "Jet Dive", "Blitz Shot"}, AttackRange = 50, KiteRange = 60 },
      ["Atomic Samurai"] = { Moves = {"Quick Slice", "Atmos Cleave", "Pinpoint Cut"}, AttackRange = 40, KiteRange = 50 },
      ["Sonic"] = { Moves = {"Flash Strike", "Explosive Shuriken"}, AttackRange = 45, KiteRange = 55 },
      ["Flashy Flash"] = { Moves = {"Split Second Counter"}, AttackRange = 38, KiteRange = 48 },
      ["Default"] = { Moves = {
          "Homerun", "Beatdown", "Grand Slam", "Flash Strike", "Whirlwind Kick",
          "Scatter", "Explosive Shuriken", "Machine Gun Blows", "Ignition Burst",
          "Blitz Shot", "Jet Dive", "Quick Slice", "Atmos Cleave", "Pinpoint Cut",
          "Split Second Counter", "Flowing Water", "Crushing Pull", "Windstorm Fury",
          "Stone Coffin", "Expulsive Push"
          }, AttackRange = 30, KiteRange = 40 }
  }

  local function IdentifyCharacter()
      local ownedMovesSet = {}
      local ownedMovesList = {}

      local function getTools(container)
          if not container then return end
          for _, tool in ipairs(container:GetChildren()) do
              if tool:IsA("Tool") and not ownedMovesSet[tool.Name] then
                  ownedMovesSet[tool.Name] = true
                  table.insert(ownedMovesList, tool.Name)
              end
          end
      end

      getTools(player:WaitForChild("Backpack"))
      getTools(character)

      for characterName, data in pairs(CHARACTER_DATA) do
          if characterName ~= "Default" then
              local moveMatch = false
              for _, moveName in ipairs(data.Moves) do
                  if ownedMovesSet[moveName] then
                      moveMatch = true
                      break
                  end
              end
              if moveMatch then
                  AI.IdentifiedCharacter, AI.AvailableMoves = characterName, ownedMovesList
                  print("Character identified as:", characterName)
                  return CHARACTER_DATA[characterName]
              end
          end
      end

      AI.IdentifiedCharacter, AI.AvailableMoves = "Custom", ownedMovesList
      print("Custom character detected. Using owned moves:", table.concat(ownedMovesList, ", "))
      return CHARACTER_DATA.Default
  end

  local characterInfo = IdentifyCharacter()

  --// --- CONSTANTS ---
  -- Centralized constants for easy tuning of the AI's behavior.
  local CONST = {
      AGGRO_RANGE = 200, ATTACK_RANGE = characterInfo.AttackRange or 30, KITE_RANGE = characterInfo.KiteRange or 40,
      TARGET_SCAN_INTERVAL = 0.3, -- Reduced from 0.5 for faster target acquisition
      MAX_ENERGY = 500, ENERGY_REGEN_RATE = 100,
      COSTS = { SPECIAL = 10, DASH = 2, FEINT = 1, ATTACK = 0, REPOSITION = 0 },
      ATTACK_COOLDOWN = 0.02, SPECIAL_MOVE_COOLDOWN = 0.35, DASH_COOLDOWN = 0.06, FEINT_COOLDOWN = 0.85, -- Ultra-fast reactions
      FEINT_DURATION = 0.18, FEINT_SUCCESS_DISTANCE = 5,
      PERFECT_DODGE_WINDOW = 0.5, PUNISH_WINDOW_DURATION = 2.0, COMBO_WINDOW_DURATION = 2.0,
      BASE_PREDICTION_TIME = 0.02, -- Optimized for ultra-fast prediction
      -- Enhanced Double Q-Learning parameters
      LEARNING_RATE_ALPHA = 0.45, -- Adaptive base rate
      MIN_LEARNING_RATE = 0.1, -- Minimum learning rate for stability
      MAX_LEARNING_RATE = 0.7, -- Maximum learning rate for fast adaptation
      DISCOUNT_FACTOR_GAMMA = 0.96, -- Improved for better long-term planning
      ELIGIBILITY_LAMBDA = 0.85, -- Enhanced eligibility trace decay
      COST_PENALTY_FACTOR = 0.08,
      -- Hybrid learning parameters
      MODEL_LEARNING_RATE = 0.3, -- For model-based learning
      MODEL_UPDATE_THRESHOLD = 10, -- Update model every N experiences
      -- Meta-learning parameters
      META_ADAPTATION_RATE = 0.05, -- How fast to adjust learning rate
      PERFORMANCE_WINDOW = 20, -- Number of actions to evaluate performance
      -- Advanced exploration strategies
      INITIAL_EPSILON = 0.25, -- Start with 25% exploration
      MIN_EPSILON = 0.03, -- Minimum 3% exploration
      EPSILON_DECAY = 0.9997, -- Slower decay for better exploration
      UCB_EXPLORATION_CONSTANT = 2.0, -- Upper Confidence Bound parameter
      BOLTZMANN_TEMPERATURE = 1.5, -- Temperature for Boltzmann exploration
      TEMPERATURE_DECAY = 0.998, -- Cool down over time
      -- Experience replay
      REPLAY_BATCH_SIZE = 8, -- Learn from multiple experiences at once
      REPLAY_FREQUENCY = 5, -- Replay every N actions
      MIN_REPLAY_SIZE = 50, -- Minimum experiences before replay starts
      N_STEP_RETURN = 3, -- Look ahead 3 steps for better credit assignment
      -- Scoring constants
      UNTRIED_ACTION_SCORE = 100, -- Initial score for unexplored actions
      REPLAY_LEARNING_RATE_MULTIPLIER = 0.5, -- Use half learning rate for replay
      -- Rewards tuning
      REWARD_DAMAGE_DEALT_MULTIPLIER = 2.5, -- Increased to encourage damage
      REWARD_PERFECT_DODGE = 30, -- Increased reward for perfect dodges
      REWARD_FEINT_SUCCESS = 12,
      PENALTY_DAMAGE_TAKEN_MULTIPLIER = -3.0, -- Stronger penalty for taking damage
      PENALTY_FAILED_ACTION = -3,
      TACTIC_CONFIDENCE_THRESHOLD = 50, TACTIC_CHANGE_COOLDOWN = 1.5, -- Reduced for faster adaptation
      EXPLOIT_SEEKER_CONFIDENCE_THRESHOLD = 20,
      EXPLOIT_SEEKER_COOLDOWN = 12.0, -- Reduced cooldown
      REWARD_ANOMALOUS_DAMAGE = 60, -- Increased to encourage glitch exploitation
      GLITCH_VELOCITY_THRESHOLD = 350,
      GLITCH_DETECTION_REWARD = 20, -- Increased glitch detection reward
      GLITCH_COMBO_WINDOW_DURATION = 1.5,
  }

  --// --- STATE MANAGEMENT ---
  function AI.ResetState(maxEnergy)
      AI.State = {
          currentTarget = nil, targetHumanoid = nil, targetTorso = nil,
          combatState = "IDLE", currentTactic = "BaitAndPunish",
          lastActionTimes = { ATTACK = 0, SPECIAL = 0, EVADE = 0, FEINT = 0 },
          strafeDirection = 1, lastStrafeChange = 0, lastAction = "IDLE",
          lastTargetScanTime = 0, currentEnergy = maxEnergy,
          lastTargetHealth = 0, myHealth = humanoid.Health,
          isFeinting = false, feintStartTime = 0, distanceAtFeintStart = 0,
          isPerfectDodging = false, dodgeStartTime = 0,
          punishWindowEndTime = 0, comboStateEndTime = 0,
          glitchComboWindowEndTime = 0,
          tacticConfidence = 100, lastTacticChangeTime = 0,
          isTargetAttacking = false,
          lastExploitSeekerTriggerTime = 0,
          -- New fields for enhanced learning
          epsilon = CONST.INITIAL_EPSILON,
          actionsSinceReplay = 0,
          stateHistory = {}, -- For n-step returns
          cachedPredictedPos = nil, -- Cache predicted position
          lastPredictionTime = 0,
          -- Advanced exploration state
          explorationStrategy = "epsilon-greedy", -- Can be: epsilon-greedy, ucb, boltzmann
          boltzmannTemperature = CONST.BOLTZMANN_TEMPERATURE,
          -- Hybrid learning state
          lastModelUpdate = 0,
          modelBasedPlanDepth = 3,
          -- Opponent tracking
          lastOpponentAction = "UNKNOWN",
          opponentActionTime = 0,
      }
      print("AI State reset. Learned statistics and score have been preserved.")
  end
  AI.ResetState(CONST.MAX_ENERGY)
  AI.GlitchScanner.lastHumanoidState = humanoid:GetState()

  print(`Initializing Project Apex Ultra (V35.0) for {player.Name}... Character: {AI.IdentifiedCharacter}. Double Q-Learning, Hybrid Learning, Opponent Modeling & Ultra-Fast Reactions active.`)

  --// ================= CORE AI LOGIC ================= //

  --// --- LEARNING MODULE ---
  
  -- Store experience in replay buffer
  function AI.Learning.StoreExperience(state, action, reward, nextState, tactic)
      local buffer = AI.ExperienceBuffer
      local experience = {
          state = state,
          action = action,
          reward = reward,
          nextState = nextState,
          tactic = tactic,
          timestamp = os.clock()
      }
      
      -- Calculate priority based on absolute TD error estimate
      local priority = math.abs(reward) + 1
      
      if #buffer.experiences < buffer.maxSize then
          table.insert(buffer.experiences, experience)
          table.insert(buffer.priorities, priority)
      else
          buffer.experiences[buffer.nextIndex] = experience
          buffer.priorities[buffer.nextIndex] = priority
      end
      
      buffer.nextIndex = (buffer.nextIndex % buffer.maxSize) + 1
  end
  
  -- Sample experiences with priority
  function AI.Learning.SampleExperiences(batchSize)
      local buffer = AI.ExperienceBuffer
      if #buffer.experiences < CONST.MIN_REPLAY_SIZE then return {} end
      
      local samples = {}
      local totalPriority = 0
      for _, p in ipairs(buffer.priorities) do
          totalPriority += p
      end
      
      for i = 1, math.min(batchSize, #buffer.experiences) do
          local rand = math.random() * totalPriority
          local cumSum = 0
          for idx, priority in ipairs(buffer.priorities) do
              cumSum += priority
              if cumSum >= rand then
                  table.insert(samples, buffer.experiences[idx])
                  break
              end
          end
      end
      
      return samples
  end
  
  -- Double Q-Learning with eligibility traces (prevents overestimation)
  function AI.Learning.Reinforce(tactic, action, rawReward, nextMaxQValue)
      local tacticStatsA = AI.ActionStats[tactic]
      local tacticStatsB = AI.QNetworkB[tactic]
      if not tacticStatsA or not tacticStatsA[action] or not tacticStatsB or not tacticStatsB[action] then return end

      -- Meta-learning: Track rewards and adapt learning rate
      table.insert(AI.MetaLearning.recentRewards, rawReward)
      if #AI.MetaLearning.recentRewards > AI.MetaLearning.maxRewardHistory then
          table.remove(AI.MetaLearning.recentRewards, 1)
      end
      
      -- Calculate performance trend for adaptive learning rate
      if #AI.MetaLearning.recentRewards >= 10 then
          local recentSum = 0
          for i = #AI.MetaLearning.recentRewards - 9, #AI.MetaLearning.recentRewards do
              recentSum += AI.MetaLearning.recentRewards[i]
          end
          local oldSum = 0
          for i = 1, math.min(10, #AI.MetaLearning.recentRewards - 10) do
              oldSum += AI.MetaLearning.recentRewards[i]
          end
          AI.MetaLearning.performanceTrend = recentSum - oldSum
      end
      
      -- Adaptive learning rate based on performance
      local learningRate = AI.MetaLearning.adaptiveLearningRate
      if AI.MetaLearning.performanceTrend > 0 then
          -- Performance improving: slightly increase learning rate
          learningRate = math.min(CONST.MAX_LEARNING_RATE, learningRate * (1 + CONST.META_ADAPTATION_RATE))
      elseif AI.MetaLearning.performanceTrend < -5 then
          -- Performance declining: decrease learning rate for stability
          learningRate = math.max(CONST.MIN_LEARNING_RATE, learningRate * (1 - CONST.META_ADAPTATION_RATE))
      end
      AI.MetaLearning.adaptiveLearningRate = learningRate

      -- Randomly choose which Q-network to update (Double Q-Learning)
      local updateA = math.random() < 0.5
      local statsToUpdate = updateA and tacticStatsA[action] or tacticStatsB[action]
      
      local actionCost = CONST.COSTS[action] or 0
      local netReward = rawReward - (actionCost * CONST.COST_PENALTY_FACTOR)

      -- Calculate TD error (nextMaxQValue already uses both networks via GetBestNextQValue)
      local old_value = statsToUpdate.value
      local td_error = netReward + CONST.DISCOUNT_FACTOR_GAMMA * nextMaxQValue - old_value
      
      -- Update eligibility trace
      statsToUpdate.eligibility = statsToUpdate.eligibility * CONST.DISCOUNT_FACTOR_GAMMA * CONST.ELIGIBILITY_LAMBDA + 1
      
      -- Q-Learning update with adaptive learning rate and eligibility traces
      statsToUpdate.value = old_value + learningRate * td_error * statsToUpdate.eligibility
      
      -- Decay eligibility traces for all actions in both networks
      for actionName, actionStats in pairs(tacticStatsA) do
          if type(actionStats) == "table" and actionStats.eligibility then
              if actionName ~= action then
                  actionStats.eligibility = actionStats.eligibility * CONST.DISCOUNT_FACTOR_GAMMA * CONST.ELIGIBILITY_LAMBDA
              end
          end
      end
      for actionName, actionStats in pairs(tacticStatsB) do
          if type(actionStats) == "table" and actionStats.eligibility then
              if actionName ~= action then
                  actionStats.eligibility = actionStats.eligibility * CONST.DISCOUNT_FACTOR_GAMMA * CONST.ELIGIBILITY_LAMBDA
              end
          end
      end

      statsToUpdate.count += 1
      tacticStatsA.total += 1
      AI.ActionStats.totalTrials += 1

      -- Adjust tactic confidence based on performance with smoothing
      local confidenceChange = rawReward * 0.4
      if rawReward > 0 then
          AI.State.tacticConfidence = math.min(150, AI.State.tacticConfidence + confidenceChange)
      else
          AI.State.tacticConfidence = math.max(0, AI.State.tacticConfidence + confidenceChange)
      end
      
      -- Update epsilon for exploration-exploitation
      AI.State.epsilon = math.max(CONST.MIN_EPSILON, AI.State.epsilon * CONST.EPSILON_DECAY)
      
      -- Update Boltzmann temperature
      AI.State.boltzmannTemperature = math.max(0.1, AI.State.boltzmannTemperature * CONST.TEMPERATURE_DECAY)
  end
  
  -- Perform experience replay learning
  function AI.Learning.ReplayExperiences()
      local samples = AI.Learning.SampleExperiences(CONST.REPLAY_BATCH_SIZE)
      
      for _, experience in ipairs(samples) do
          local tacticStats = AI.ActionStats[experience.tactic]
          if tacticStats and tacticStats[experience.action] then
              -- Calculate best Q-value for next state
              local nextMaxQ = AI.Learning.GetBestNextQValue(experience.tactic, {
                  ATTACK = true, SPECIAL = true, EVADE = true, FEINT = true, REPOSITION = true
              })
              
              -- Perform Q-learning update
              local stats = tacticStats[experience.action]
              local old_value = stats.value
              local td_error = experience.reward + CONST.DISCOUNT_FACTOR_GAMMA * nextMaxQ - old_value
              stats.value = old_value + CONST.LEARNING_RATE_ALPHA * CONST.REPLAY_LEARNING_RATE_MULTIPLIER * td_error
          end
      end
  end

  function AI.Learning.GetBestNextQValue(tactic, possibleActions)
      local tacticStatsA = AI.ActionStats[tactic]
      local tacticStatsB = AI.QNetworkB[tactic]
      if not tacticStatsA or not tacticStatsB then return 0 end

      -- Double Q-Learning: Use average of both networks to reduce overestimation
      local maxQ = -math.huge
      local foundAction = false
      for action, available in pairs(possibleActions) do
          if available then
              local statsA = tacticStatsA[action]
              local statsB = tacticStatsB[action]
              if statsA and statsA.count > 0 and statsB and statsB.count > 0 then
                  local qValueA = statsA.value / statsA.count
                  local qValueB = statsB.value / statsB.count
                  -- Average both Q-networks (Double Q-Learning)
                  local qValue = (qValueA + qValueB) / 2
                  if qValue > maxQ then
                      maxQ = qValue
                      foundAction = true
                  end
              elseif statsA and statsA.count > 0 then
                  local qValue = statsA.value / statsA.count
                  if qValue > maxQ then
                      maxQ = qValue
                      foundAction = true
                  end
              end
          end
      end
      return foundAction and maxQ or 0
  end
  
  -- Enhanced deep feature extraction with more sophisticated state representation
  function AI.Learning.ExtractFeatures(state)
      -- Basic features
      local healthAdvantage = (state.myHealthPercent - state.targetHealthPercent) * AI.FeatureWeights.healthDiff
      local distanceNorm = (1 - math.min(state.distance, CONST.AGGRO_RANGE) / CONST.AGGRO_RANGE) * AI.FeatureWeights.distanceToTarget
      local energyNorm = (state.energy / CONST.MAX_ENERGY) * AI.FeatureWeights.energyLevel
      
      -- Advanced features
      local inRangeScore = (state.inRange and 1 or 0) * 1.5
      local threatLevel = (state.isTargetAttacking and 1 or 0) * AI.FeatureWeights.threatLevel
      
      -- Combo potential
      local now = os.clock()
      local comboActive = (now < AI.State.comboStateEndTime) and 1 or 0
      local comboDensity = comboActive * AI.FeatureWeights.comboDensity
      
      -- Opportunity windows
      local opportunityScore = 0
      if now < AI.State.punishWindowEndTime then
          opportunityScore += 2.0
      end
      if now < AI.State.glitchComboWindowEndTime then
          opportunityScore += 2.5
      end
      opportunityScore = opportunityScore * AI.FeatureWeights.opportunityWindow
      
      -- Opponent aggression modeling
      local opponentThreat = AI.OpponentModel.aggressionScore * AI.FeatureWeights.threatLevel
      
      local features = {
          healthAdvantage = healthAdvantage,
          distanceNorm = distanceNorm,
          energyNorm = energyNorm,
          inRangeScore = inRangeScore,
          threatLevel = threatLevel,
          comboDensity = comboDensity,
          opportunityScore = opportunityScore,
          opponentThreat = opponentThreat,
      }
      
      -- Compute weighted feature score
      local score = 0
      for _, value in pairs(features) do
          score += value
      end
      
      -- Normalize score dynamically based on number of features
      local featureCount = 0
      for _ in pairs(features) do featureCount += 1 end
      score = featureCount > 0 and (score / featureCount) or 0
      
      return features, score
  end
  
  -- Update opponent model based on observed actions
  function AI.Learning.UpdateOpponentModel(observedAction)
      if observedAction and observedAction ~= "UNKNOWN" then
          -- Update action frequency
          if AI.OpponentModel.actionFrequency[observedAction] then
              AI.OpponentModel.actionFrequency[observedAction] += 1
          end
          
          -- Add to history
          table.insert(AI.OpponentModel.lastActions, observedAction)
          if #AI.OpponentModel.lastActions > AI.OpponentModel.maxHistorySize then
              table.remove(AI.OpponentModel.lastActions, 1)
          end
          
          -- Calculate aggression score based on action patterns
          local aggressiveActions = 0
          local defensiveActions = 0
          for _, act in ipairs(AI.OpponentModel.lastActions) do
              if act == "ATTACK" or act == "SPECIAL" then
                  aggressiveActions += 1
              elseif act == "EVADE" or act == "FEINT" then
                  defensiveActions += 1
              end
          end
          
          if #AI.OpponentModel.lastActions > 0 then
              AI.OpponentModel.aggressionScore = aggressiveActions / #AI.OpponentModel.lastActions
          end
          
          -- Predict next action based on pattern (improved sequence analysis)
          if #AI.OpponentModel.lastActions >= 3 then
              -- Look for action patterns in recent history
              local last1 = AI.OpponentModel.lastActions[#AI.OpponentModel.lastActions]
              local last2 = AI.OpponentModel.lastActions[#AI.OpponentModel.lastActions - 1]
              
              -- Simple pattern matching: if last 2 actions were aggressive, predict aggression continues
              if (last1 == "ATTACK" or last1 == "SPECIAL") and (last2 == "ATTACK" or last2 == "SPECIAL") then
                  AI.OpponentModel.predictedNextAction = "ATTACK"
              elseif (last1 == "EVADE" or last1 == "FEINT") and (last2 == "EVADE" or last2 == "FEINT") then
                  AI.OpponentModel.predictedNextAction = "EVADE"
              else
                  -- Default to most frequent action
                  local maxFreq = 0
                  local mostFrequent = "ATTACK"
                  for action, freq in pairs(AI.OpponentModel.actionFrequency) do
                      if freq > maxFreq then
                          maxFreq = freq
                          mostFrequent = action
                      end
                  end
                  AI.OpponentModel.predictedNextAction = mostFrequent
              end
          end
      end
  end
  
  -- Hybrid learning: Update transition model
  function AI.Learning.UpdateTransitionModel(state, action, nextState, reward)
      if not AI.HybridModel.useModelBased then return end
      
      -- Create state signature (simplified)
      local stateKey = string.format("H%.1f_D%.0f_E%.0f", 
          state.myHealthPercent or 0, 
          state.distance or 0, 
          state.energy or 0)
      local nextStateKey = string.format("H%.1f_D%.0f_E%.0f", 
          nextState.myHealthPercent or 0, 
          nextState.distance or 0, 
          nextState.energy or 0)
      
      -- Update transition counts
      local transitionKey = stateKey .. "_" .. action
      if not AI.HybridModel.transitionCounts[transitionKey] then
          AI.HybridModel.transitionCounts[transitionKey] = {}
      end
      AI.HybridModel.transitionCounts[transitionKey][nextStateKey] = 
          (AI.HybridModel.transitionCounts[transitionKey][nextStateKey] or 0) + 1
      
      -- Update reward model
      if not AI.HybridModel.rewardModel[transitionKey] then
          AI.HybridModel.rewardModel[transitionKey] = { sum = 0, count = 0 }
      end
      AI.HybridModel.rewardModel[transitionKey].sum += reward
      AI.HybridModel.rewardModel[transitionKey].count += 1
      
      -- Update model confidence based on data collected
      local totalTransitions = 0
      for _, nextStates in pairs(AI.HybridModel.transitionCounts) do
          for _, count in pairs(nextStates) do
              totalTransitions += count
          end
      end
      AI.HybridModel.modelConfidence = math.min(0.8, totalTransitions / 100)
  end
  
  -- Model-based planning: Simulate future outcomes
  function AI.Learning.SimulateFuture(state, action, tactic, depth)
      if depth <= 0 or not AI.HybridModel.useModelBased then return 0 end
      
      local stateKey = string.format("H%.1f_D%.0f_E%.0f", 
          state.myHealthPercent or 0, 
          state.distance or 0, 
          state.energy or 0)
      local transitionKey = stateKey .. "_" .. action
      
      -- Get expected reward from model
      local expectedReward = 0
      if AI.HybridModel.rewardModel[transitionKey] then
          local model = AI.HybridModel.rewardModel[transitionKey]
          expectedReward = model.sum / model.count
      end
      
      -- Simple recursive planning (limited depth)
      if depth > 1 and AI.HybridModel.transitionCounts[transitionKey] then
          local futureValue = 0
          local totalCount = 0
          
          -- Average over possible next states
          for nextStateKey, count in pairs(AI.HybridModel.transitionCounts[transitionKey]) do
              totalCount += count
          end
          
          if totalCount > 0 then
              futureValue = AI.Learning.GetBestNextQValue(tactic, {
                  ATTACK = true, SPECIAL = true, EVADE = true, FEINT = true, REPOSITION = true
              })
          end
          
          return expectedReward + CONST.DISCOUNT_FACTOR_GAMMA * futureValue
      end
      
      return expectedReward
  end

  --// --- EVALUATION MODULE ---
  function AI.Evaluation.ChooseTactic(state)
      local now = os.clock()

      -- Highest priority: Glitch exploitation.
      if AI.GlitchScanner.isGlitchDetected and now - AI.GlitchScanner.detectionTime < 5.0 then
          if AI.State.currentTactic ~= "ExploitSeeker" then
              print(`GLITCH DETECTED (${AI.GlitchScanner.glitchType})! Forcing EXPLOITSEEKER tactic.`)
              -- Reinforce the action that *caused* the glitch.
              AI.Learning.Reinforce("ExploitSeeker", AI.State.lastAction, CONST.GLITCH_DETECTION_REWARD, 0) -- No future Q-value needed here.
          end
          return "ExploitSeeker"
      end

      -- Cooldown to prevent rapid tactic switching.
      if now - AI.State.lastTacticChangeTime < CONST.TACTIC_CHANGE_COOLDOWN then
          return AI.State.currentTactic
      end

      -- If confidence is critical, switch to experimental mode.
      if AI.State.tacticConfidence < CONST.EXPLOIT_SEEKER_CONFIDENCE_THRESHOLD and now - AI.State.lastExploitSeekerTriggerTime > CONST.EXPLOIT_SEEKER_COOLDOWN then
          AI.State.lastTacticChangeTime = now
          AI.State.lastExploitSeekerTriggerTime = now
          print("Confidence critical. Switching to EXPLOITSEEKER to find new strategies.")
          return "ExploitSeeker"
      end

      -- If confidence is high, stick with the current tactic for a bit longer.
      if AI.State.tacticConfidence > CONST.TACTIC_CONFIDENCE_THRESHOLD and os.clock() - AI.State.lastTacticChangeTime < CONST.TACTIC_CHANGE_COOLDOWN * 2 then
          return AI.State.currentTactic
      end

      -- Rule-based tactic selection.
      local newTactic
      if state.targetHealthPercent < 0.2 and state.distance < CONST.ATTACK_RANGE * 1.5 then newTactic = "Finisher"
      elseif state.myHealthPercent > 0.75 and state.energy > (CONST.MAX_ENERGY * 0.6) then newTactic = "Aggressive"
      elseif state.myHealthPercent < 0.35 or state.energy < (CONST.MAX_ENERGY * 0.25) then newTactic = "Defensive"
      else newTactic = "BaitAndPunish" end

      if newTactic ~= AI.State.currentTactic then
          AI.State.lastTacticChangeTime = now
          AI.State.tacticConfidence = 100 -- Reset confidence on switch.
          print("Tactic changed to:", newTactic)
      end
      return newTactic
  end

  function AI.Evaluation.FindBestActionByScore(state, tactic)
      local now = os.clock()

      -- Immediate capitalization windows (glitch or punish) - highest priority
      local function isCapitalizeWindow()
          return now < AI.State.glitchComboWindowEndTime or now < AI.State.punishWindowEndTime
      end

      if isCapitalizeWindow() and state.inRange then
          if AI.State.glitchComboWindowEndTime > now then print("Executing Glitch Combo!") end
          if #AI.AvailableMoves > 0 and now - AI.State.lastActionTimes.SPECIAL > CONST.SPECIAL_MOVE_COOLDOWN and state.energy >= CONST.COSTS.SPECIAL then return "SPECIAL" end
          if now - AI.State.lastActionTimes.ATTACK > CONST.ATTACK_COOLDOWN then return "ATTACK" end
      end

      -- Cache possible actions check for ultra-fast performance
      local possibleActions = {}
      local inCombo = now < AI.State.comboStateEndTime
      possibleActions.ATTACK = (now - AI.State.lastActionTimes.ATTACK > (inCombo and CONST.ATTACK_COOLDOWN * 0.5 or CONST.ATTACK_COOLDOWN)) and state.inRange
      possibleActions.SPECIAL = (now - AI.State.lastActionTimes.SPECIAL > CONST.SPECIAL_MOVE_COOLDOWN) and (state.energy >= CONST.COSTS.SPECIAL) and state.inRange and (#AI.AvailableMoves > 0)
      possibleActions.EVADE = (now - AI.State.lastActionTimes.EVADE > CONST.DASH_COOLDOWN) and (state.energy >= CONST.COSTS.DASH)
      possibleActions.FEINT = (now - AI.State.lastActionTimes.FEINT > CONST.FEINT_COOLDOWN) and (state.energy >= CONST.COSTS.FEINT) and state.inRange
      possibleActions.REPOSITION = true

      -- ExploitSeeker uses pure random exploration
      if tactic == "ExploitSeeker" then
          local actionPool = {}
          for action, available in pairs(possibleActions) do
              if available and (action == "ATTACK" or action == "SPECIAL" or action == "EVADE" or action == "FEINT") then
                  table.insert(actionPool, action)
              end
          end
          return #actionPool > 0 and actionPool[math.random(#actionPool)] or "REPOSITION"
      end

      -- Advanced exploration strategies
      local tacticStatsA = AI.ActionStats[tactic]
      local tacticStatsB = AI.QNetworkB[tactic]
      local actionScores = {}
      
      -- Extract deep features for better decision-making
      local features, featureScore = AI.Learning.ExtractFeatures(state)
      
      -- Choose exploration strategy based on learning progress
      local explorationMode = AI.State.explorationStrategy
      if AI.ActionStats.totalTrials < 100 then
          explorationMode = "epsilon-greedy" -- Early exploration
      elseif AI.ActionStats.totalTrials < 500 then
          explorationMode = "ucb" -- Mid-game exploration
      else
          explorationMode = "boltzmann" -- Late-game refinement
      end

      -- Build action scores with hybrid model integration
      for action, available in pairs(possibleActions) do
          if available then
              local statsA = tacticStatsA[action]
              local statsB = tacticStatsB[action]
              local score
              
              if statsA.count == 0 then
                  score = CONST.UNTRIED_ACTION_SCORE
              else
                  -- Double Q-Learning: Average both networks with safety checks
                  local qValueA = statsA.value / statsA.count
                  local qValueB = qValueA -- Default to A if B not available
                  if statsB and statsB.count and statsB.value and statsB.count > 0 then
                      qValueB = statsB.value / statsB.count
                  end
                  local qValue = (qValueA + qValueB) / 2
                  
                  -- Hybrid learning: Combine model-free and model-based
                  if AI.HybridModel.useModelBased and AI.HybridModel.modelConfidence > 0.2 then
                      local modelValue = AI.Learning.SimulateFuture(state, action, tactic, AI.State.modelBasedPlanDepth)
                      qValue = qValue * (1 - AI.HybridModel.modelConfidence) + modelValue * AI.HybridModel.modelConfidence
                  end
                  
                  -- Add deep feature score
                  score = qValue + featureScore * 0.15
              end

              -- Apply situational modifiers for tactical awareness
              local modifier = 1.0
              
              -- React to predicted opponent action
              if AI.OpponentModel.predictedNextAction == "ATTACK" and action == "EVADE" then
                  modifier *= 1.5
              end
              
              if action == "EVADE" and state.isTargetAttacking then 
                  modifier = 2.2 -- Stronger preference for evasion when under attack
              end
              if (tactic == "BaitAndPunish" or tactic == "Defensive") and action == "EVADE" then 
                  modifier *= 1.4
              end
              if action == "ATTACK" and inCombo then
                  modifier *= 1.5 -- Stronger combo encouragement
              end
              if action == "SPECIAL" and now < AI.State.punishWindowEndTime then
                  modifier *= 1.6 -- Capitalize on punish windows
              end

              table.insert(actionScores, { 
                  action = action, 
                  score = score * modifier,
                  count = statsA.count,
                  rawQ = (statsA.count > 0) and (statsA.value / statsA.count) or 0
              })
          end
      end

      if #actionScores == 0 then return "REPOSITION" end

      -- Apply exploration strategy
      if explorationMode == "epsilon-greedy" and math.random() < AI.State.epsilon then
          -- Epsilon-greedy: Random action
          return actionScores[math.random(#actionScores)].action
      elseif explorationMode == "ucb" then
          -- Upper Confidence Bound: Balance exploration and exploitation
          for _, actionData in ipairs(actionScores) do
              if actionData.count > 0 then
                  local exploration = CONST.UCB_EXPLORATION_CONSTANT * math.sqrt(math.log(AI.ActionStats.totalTrials) / actionData.count)
                  actionData.score += exploration
              end
          end
      elseif explorationMode == "boltzmann" then
          -- Boltzmann exploration: Probabilistic selection based on Q-values
          local expScores = {}
          local sumExp = 0
          for _, actionData in ipairs(actionScores) do
              local expScore = math.exp(actionData.score / AI.State.boltzmannTemperature)
              table.insert(expScores, expScore)
              sumExp += expScore
          end
          
          -- Probabilistic selection
          local rand = math.random() * sumExp
          local cumSum = 0
          for i, expScore in ipairs(expScores) do
              cumSum += expScore
              if cumSum >= rand then
                  return actionScores[i].action
              end
          end
      end

      -- Sort by score (highest first) and return best
      table.sort(actionScores, function(a, b) return a.score > b.score end)
      return actionScores[1].action
  end

  --// --- ACTION MODULE ---
  function AI.Actions.Execute(action, state)
      local now = os.clock()
      AI.State.lastAction = action

      if not AI.State.targetTorso then return end -- Safety check
      local targetPos = AI.State.targetTorso.Position

      local cost = CONST.COSTS[action:upper()] or 0
      AI.State.currentEnergy -= cost
      
      -- Store current state for experience replay
      AI.State.actionsSinceReplay += 1

      if action == "FEINT" then
          communicateEvent:FireServer({["Goal"]="Feint"})
          AI.State.lastActionTimes.FEINT = now
          AI.State.isFeinting, AI.State.feintStartTime, AI.State.distanceAtFeintStart = true, now, (rootPart.Position - targetPos).Magnitude
      elseif action == "ATTACK" then
          communicateEvent:FireServer({["Mobile"]=true, ["Goal"]="LeftClick"})
          AI.State.lastActionTimes.ATTACK = now
          AI.State.punishWindowEndTime = 0
      elseif action == "SPECIAL" then
          if #AI.AvailableMoves > 0 then
              local moveName = AI.AvailableMoves[math.random(#AI.AvailableMoves)]
              local tool = player.Backpack:FindFirstChild(moveName) or character:FindFirstChild(moveName)
              if tool then
                  communicateEvent:FireServer({["Tool"] = tool, ["Goal"] = "Console Move"})
                  AI.State.lastActionTimes.SPECIAL = now
                  AI.State.punishWindowEndTime = 0
              end
          end
      elseif action == "EVADE" then
          -- Smarter evade direction based on target velocity
          local targetVelocity = AI.State.targetTorso.AssemblyLinearVelocity
          local perpendicular = rootPart.CFrame.RightVector * (math.random() > 0.5 and 1 or -1)
          if targetVelocity.Magnitude > 10 then
              -- Dodge perpendicular to target's movement (+90 degree rotation in horizontal plane)
              local targetDir = targetVelocity.Unit
              perpendicular = Vector3.new(-targetDir.Z, 0, targetDir.X)
          end
          communicateEvent:FireServer({["Dash"]=perpendicular.Unit, ["Key"]="Q", ["Goal"]="KeyPress"})
          AI.State.lastActionTimes.EVADE = now
          AI.State.isPerfectDodging, AI.State.dodgeStartTime = true, now
      elseif action == "REPOSITION" then
          if now - AI.State.lastStrafeChange > (0.8 + math.random() * 0.4) then -- Faster strafe changes
              AI.State.strafeDirection *= -1
              AI.State.lastStrafeChange = now
          end

          local currentDist = (rootPart.Position - targetPos).Magnitude
          local desiredRange
          if AI.State.currentTactic == "Defensive" then desiredRange = CONST.KITE_RANGE
          elseif AI.State.currentTactic == "Finisher" or AI.State.currentTactic == "Aggressive" then desiredRange = CONST.ATTACK_RANGE * 0.7
          else desiredRange = CONST.ATTACK_RANGE * 0.9 end

          local moveDirection = (targetPos - ((targetPos - rootPart.Position).Unit * desiredRange) + (rootPart.CFrame.RightVector * 10 * AI.State.strafeDirection))
          humanoid:MoveTo(moveDirection)

          -- Immediate feedback for repositioning
          local distanceError = math.abs(currentDist - desiredRange)
          local reward = math.max(0, 1 - (distanceError / desiredRange)) * 2
          AI.Learning.Reinforce(AI.State.currentTactic, "REPOSITION", reward, 0)
      end
      
      -- Store experience for replay learning and hybrid model
      if state then
          AI.Learning.StoreExperience(state, action, 0, state, AI.State.currentTactic)
          
          -- Update hybrid transition model
          if AI.ActionStats.totalTrials % CONST.MODEL_UPDATE_THRESHOLD == 0 then
              AI.Learning.UpdateTransitionModel(state, action, state, 0)
          end
      end
      
      -- Perform experience replay periodically
      if AI.State.actionsSinceReplay >= CONST.REPLAY_FREQUENCY then
          AI.Learning.ReplayExperiences()
          AI.State.actionsSinceReplay = 0
      end
  end

  --// ================= HELPER & UPDATE FUNCTIONS ================= //

  local function Cleanup()
      if targetDiedConnection then targetDiedConnection:Disconnect(); targetDiedConnection = nil end
      for _, c in ipairs(connections) do c:Disconnect() end
      connections = {}
      AI.State.combatState = "IDLE"
      AI.State.currentTarget = nil
      print("Project Apex has been shut down for this character.")
  end

  local function GetTargetScore(targetCharacter)
      -- Fast validity checks with early returns
      if not targetCharacter or not targetCharacter.Parent then return 0 end
      local root = targetCharacter:FindFirstChild("HumanoidRootPart")
      if not root then return 0 end
      
      local targetHumanoid = targetCharacter:FindFirstChildOfClass("Humanoid")
      if not targetHumanoid or targetHumanoid.Health <= 0 then return 0 end

      -- Pre-calculate distance once
      local distance = (rootPart.Position - root.Position).Magnitude
      if distance > CONST.AGGRO_RANGE then return 0 end

      -- Optimized scoring with weighted factors
      local healthRatio = targetHumanoid.Health / targetHumanoid.MaxHealth
      local healthScore = (1 - healthRatio) * 2.0 -- Increased weight for low-health targets
      local distanceScore = (1 - (distance / CONST.AGGRO_RANGE)) * 1.5 -- Closer is better
      
      -- Improved velocity penalty with gradual scaling
      local velocityMag = root.AssemblyLinearVelocity.Magnitude
      local velocityScore = velocityMag > 25 and -(velocityMag / 50) or 0.2 -- Bonus for stationary targets

      return healthScore + distanceScore + velocityScore
  end

  local function EvaluateAndSetTarget()
      local bestTarget, highestScore = nil, -math.huge
      for _, p in ipairs(Players:GetPlayers()) do
          if p ~= player and p.Character then
              local score = GetTargetScore(p.Character)
              if score > highestScore then
                  highestScore, bestTarget = score, p.Character
              end
          end
      end

      if bestTarget and bestTarget ~= AI.State.currentTarget then
          if targetDiedConnection then targetDiedConnection:Disconnect() end

          local targetHumanoid = bestTarget:FindFirstChildOfClass("Humanoid")
          AI.State.currentTarget, AI.State.targetHumanoid, AI.State.targetTorso = bestTarget, targetHumanoid, bestTarget:FindFirstChild("Torso") or bestTarget:FindFirstChild("HumanoidRootPart")
          AI.State.lastTargetHealth = targetHumanoid and targetHumanoid.Health or 0

          targetDiedConnection = targetHumanoid.Died:Connect(function()
              AI.Score += 10
              print(`Target eliminated. New Score: {AI.Score}`)
              AI.State.currentTarget = nil
          end)
      end

      if not AI.State.currentTarget or not AI.State.targetHumanoid or AI.State.targetHumanoid.Health <= 0 then
          AI.State.combatState = "IDLE"
          AI.State.currentTarget = nil
      else
          AI.State.combatState = "ENGAGING"
      end
  end

  -- Helper function to detect NaN values (IEEE 754: NaN is the only value that doesn't equal itself)
  local function isNaN(value)
      return value ~= value
  end

  local function RunGlitchScanner()
      local now = os.clock()
      
      -- Quick exit if glitch is already active
      if AI.GlitchScanner.isGlitchDetected then
          if now - AI.GlitchScanner.detectionTime > 2.0 then
              AI.GlitchScanner.isGlitchDetected = false
          end
          return
      end

      local function triggerGlitch(glitchType)
          AI.GlitchScanner.isGlitchDetected = true
          AI.GlitchScanner.glitchType = glitchType
          AI.GlitchScanner.detectionTime = now
          AI.State.glitchComboWindowEndTime = now + CONST.GLITCH_COMBO_WINDOW_DURATION
          print(`GLITCH DETECTED: {glitchType}`)
      end

      -- 1. Fast velocity spike detection (optimized)
      local currentVelocity = rootPart.AssemblyLinearVelocity
      local currentVelocityMag = currentVelocity.Magnitude
      if currentVelocityMag > CONST.GLITCH_VELOCITY_THRESHOLD and AI.GlitchScanner.lastVelocityMagnitude < CONST.GLITCH_VELOCITY_THRESHOLD then
          triggerGlitch("Velocity Spike")
          AI.GlitchScanner.lastVelocityMagnitude = currentVelocityMag
          return
      end
      AI.GlitchScanner.lastVelocityMagnitude = currentVelocityMag

      -- 2. Humanoid state anomaly detection (optimized with early return)
      local currentState = humanoid:GetState()
      if currentState ~= AI.GlitchScanner.lastHumanoidState then
          if currentState == Enum.HumanoidStateType.StrafingNoPhysics or currentState == Enum.HumanoidStateType.None then
              triggerGlitch("Humanoid State Anomaly")
              AI.GlitchScanner.lastHumanoidState = currentState
              return
          end
          AI.GlitchScanner.lastHumanoidState = currentState
      end

      -- 3. NaN position check (fast)
      local pos = rootPart.Position
      if isNaN(pos.X) or isNaN(pos.Y) or isNaN(pos.Z) then
          triggerGlitch("NaN Position")
      end
  end

  local function UpdateAIState(deltaTime)
      local now = os.clock()
      AI.State.currentEnergy = math.min(CONST.MAX_ENERGY, AI.State.currentEnergy + CONST.ENERGY_REGEN_RATE * deltaTime)

      if now - AI.State.lastTargetScanTime > CONST.TARGET_SCAN_INTERVAL then
          EvaluateAndSetTarget()
          AI.State.lastTargetScanTime = now
      end

      if AI.State.currentTarget and AI.State.currentTarget.Parent then
          local activeTool = AI.State.currentTarget:FindFirstChildOfClass("Tool")
          local wasAttacking = AI.State.isTargetAttacking
          AI.State.isTargetAttacking = activeTool and activeTool.Enabled
          
          -- Detect opponent actions for modeling
          if AI.State.isTargetAttacking and not wasAttacking then
              AI.Learning.UpdateOpponentModel("ATTACK")
              AI.State.lastOpponentAction = "ATTACK"
              AI.State.opponentActionTime = now
          elseif AI.State.targetTorso then
              local targetVelocity = AI.State.targetTorso.AssemblyLinearVelocity.Magnitude
              if targetVelocity > 50 and now - AI.State.opponentActionTime > 0.5 then
                  AI.Learning.UpdateOpponentModel("EVADE")
                  AI.State.lastOpponentAction = "EVADE"
                  AI.State.opponentActionTime = now
              end
          end
      else
          AI.State.isTargetAttacking = false
      end

      RunGlitchScanner() -- Run the scanner every state update.
  end

  local function UpdateLearningAndRewards(now)
      -- Fast exit if no valid target
      if not AI.State.currentTarget or not AI.State.targetHumanoid or not AI.State.targetTorso then return end

      local tactic = AI.State.currentTactic

      -- Feint evaluation (optimized timing check)
      if AI.State.isFeinting and now > AI.State.feintStartTime + CONST.FEINT_DURATION then
          AI.State.isFeinting = false
          local currentDistance = (rootPart.Position - AI.State.targetTorso.Position).Magnitude
          local feintSucceeded = currentDistance > AI.State.distanceAtFeintStart + CONST.FEINT_SUCCESS_DISTANCE
          local reward = feintSucceeded and CONST.REWARD_FEINT_SUCCESS or CONST.PENALTY_FAILED_ACTION
          AI.Learning.Reinforce(tactic, "FEINT", reward, 0)
          if feintSucceeded then
              AI.State.punishWindowEndTime = now + CONST.PUNISH_WINDOW_DURATION
              print("Feint successful! Punish window opened.")
          end
      end

      -- Perfect dodge window timeout
      if AI.State.isPerfectDodging and now > AI.State.dodgeStartTime + CONST.PERFECT_DODGE_WINDOW then
          AI.State.isPerfectDodging = false
      end

      -- Damage dealt evaluation with hybrid learning and feature weight adaptation
      local currentTargetHealth = AI.State.targetHumanoid.Health
      local damageDealt = AI.State.lastTargetHealth - currentTargetHealth
      if damageDealt > 0 then
          local action = AI.State.lastAction
          local reward
          
          -- Anomaly detection for unexpected damage sources
          if action ~= "ATTACK" and action ~= "SPECIAL" then
              print(`ANOMALY: Damage (${damageDealt}) dealt via ${action}. High reward!`)
              reward = CONST.REWARD_ANOMALOUS_DAMAGE
          else
              reward = damageDealt * CONST.REWARD_DAMAGE_DEALT_MULTIPLIER
              -- Bonus for combo damage
              if now < AI.State.comboStateEndTime then
                  reward *= 1.35
              end
              -- Bonus for exploiting opportunity windows
              if now < AI.State.punishWindowEndTime then
                  reward *= 1.25
              end
          end

          -- Calculate next state Q-value for temporal difference learning (Double Q-Learning)
          local nextStateActions = { ATTACK = true, SPECIAL = true, EVADE = true, FEINT = true, REPOSITION = true }
          local nextMaxQ = AI.Learning.GetBestNextQValue(tactic, nextStateActions)
          AI.Learning.Reinforce(tactic, action, reward, nextMaxQ)

          -- Extend combo window
          AI.State.comboStateEndTime = now + CONST.COMBO_WINDOW_DURATION
          
          -- Adaptive feature weight tuning based on success
          AI.FeatureWeights.tacticSuccess = math.min(2.5, AI.FeatureWeights.tacticSuccess * 1.08)
          AI.FeatureWeights.comboDensity = math.min(2.0, AI.FeatureWeights.comboDensity * 1.05)
          AI.FeatureWeights.opportunityWindow = math.min(2.2, AI.FeatureWeights.opportunityWindow * 1.06)
          
          -- Adjust feature weights based on context
          if now < AI.State.punishWindowEndTime then
              AI.FeatureWeights.opportunityWindow = math.min(2.5, AI.FeatureWeights.opportunityWindow * 1.1)
          end
      else
          -- Decay feature weights if no damage (adaptive forgetting)
          AI.FeatureWeights.tacticSuccess = math.max(0.4, AI.FeatureWeights.tacticSuccess * 0.995)
          AI.FeatureWeights.comboDensity = math.max(0.5, AI.FeatureWeights.comboDensity * 0.997)
      end
      
      AI.State.lastTargetHealth = currentTargetHealth
  end

  local function ExecuteCombatLogic(now)
      if AI.State.combatState ~= "ENGAGING" or AI.State.isFeinting then return end
      if not (AI.State.targetTorso and AI.State.targetTorso.Parent and AI.State.targetHumanoid) then return end

      -- Ultra-optimized prediction with aggressive caching
      local shouldRecalcPrediction = not AI.State.cachedPredictedPos or (now - AI.State.lastPredictionTime) > 0.08
      if shouldRecalcPrediction then
          local ping = player:GetNetworkPing()
          local velocity = AI.State.targetTorso.AssemblyLinearVelocity
          -- Simplified prediction for speed
          AI.State.cachedPredictedPos = AI.State.targetTorso.Position + velocity * (CONST.BASE_PREDICTION_TIME + ping * 0.5)
          AI.State.lastPredictionTime = now
      end
      
      -- Ultra-fast aim update (single operation)
      local targetPos = AI.State.cachedPredictedPos
      rootPart.CFrame = CFrame.lookAt(rootPart.Position, Vector3.new(targetPos.X, rootPart.Position.Y, targetPos.Z))

      -- Pre-calculate values once for entire decision cycle
      local distance = (rootPart.Position - AI.State.targetTorso.Position).Magnitude
      local myHealthPercent = humanoid.Health / humanoid.MaxHealth
      local targetHealthPercent = AI.State.targetHumanoid.Health / AI.State.targetHumanoid.MaxHealth
      
      -- Build current state object for decision making (ultra-optimized)
      local state = {
          myHealthPercent = myHealthPercent,
          targetHealthPercent = targetHealthPercent,
          distance = distance,
          energy = AI.State.currentEnergy,
          inRange = distance <= CONST.ATTACK_RANGE,
          isTargetAttacking = AI.State.isTargetAttacking,
      }

      -- Single-pass decision pipeline
      AI.State.currentTactic = AI.Evaluation.ChooseTactic(state)
      local bestAction = AI.Evaluation.FindBestActionByScore(state, AI.State.currentTactic)
      AI.Actions.Execute(bestAction, state)
  end

  --// ================= EVENT CONNECTIONS ================= //
  table.insert(connections, humanoid.HealthChanged:Connect(function(newHealth)
      local damageTaken = AI.State.myHealth - newHealth
      if damageTaken > 0 then
          local penalty = damageTaken * CONST.PENALTY_DAMAGE_TAKEN_MULTIPLIER
          local failedAction = AI.State.lastAction
          if AI.State.isPerfectDodging and now > AI.State.dodgeStartTime then -- Check if dodge was active
              failedAction = "EVADE"
              AI.Learning.Reinforce(AI.State.currentTactic, "EVADE", CONST.PENALTY_FAILED_ACTION, 0)
          else
              AI.Learning.Reinforce(AI.State.currentTactic, failedAction, penalty, 0)
          end
          AI.State.isPerfectDodging = false
      elseif AI.State.isPerfectDodging and AI.State.isTargetAttacking then
          AI.Learning.Reinforce(AI.State.currentTactic, "EVADE", CONST.REWARD_PERFECT_DODGE, 0)
          AI.State.punishWindowEndTime = os.clock() + CONST.PUNISH_WINDOW_DURATION
          print("Perfect Dodge! Opening Punish Window.")
          AI.State.isPerfectDodging = false
      end
      AI.State.myHealth = newHealth
  end))

  table.insert(connections, LogService.MessageOut:Connect(function(message, messageType)
      if messageType == Enum.MessageType.MessageError and not AI.GlitchScanner.isGlitchDetected then
          local now = os.clock()
          AI.GlitchScanner.isGlitchDetected = true
          AI.GlitchScanner.glitchType = "Game Script Error"
          AI.GlitchScanner.detectionTime = now
          AI.State.glitchComboWindowEndTime = now + CONST.GLITCH_COMBO_WINDOW_DURATION
      end
  end))

  table.insert(connections, humanoid.Died:Connect(function()
      AI.Score -= 10
      print(`Project Apex defeated. New Score: {AI.Score}.`)
      Cleanup()
  end))
  table.insert(connections, character.AncestryChanged:Connect(function(_, parent)
      if not parent then Cleanup() end
  end))

  -- The main heartbeat loop that drives the AI.
  table.insert(connections, RunService.Heartbeat:Connect(function(deltaTime)
      local success, err = pcall(function()
          local now = os.clock()
          UpdateAIState(deltaTime)
          UpdateLearningAndRewards(now)
          ExecuteCombatLogic(now)
      end)
      if not success then
          warn("Project Apex encountered an error in its main loop:", err)
      end
  end))
end

--// ================= SCRIPT ENTRY POINT ================= //
-- Connect the initialization function to the CharacterAdded event.
player.CharacterAdded:Connect(initializeCombatSystem)
-- If the character already exists when the script runs, initialize immediately.
if player.Character then
  initializeCombatSystem(player.Character)
end
