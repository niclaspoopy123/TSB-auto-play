 -- =================================================================================
--// INSTRUCTION: This script must be placed in StarterPlayer > StarterPlayerScripts
--// to ensure it runs automatically and persists when your character changes.
-- =================================================================================
--// SELF-LEARNING COMBAT FRAMEWORK V36.0 - "PROJECT DEEP APEX" (Enhanced by GitHub Copilot)
--// USER: niclaspoopy123
--// BUILD DATE: 2025-11-01
--// UPDATE: True Deep Reinforcement Learning with neural network approximation, enhanced opponent modeling with pattern recognition, and advanced exploration strategies.
--// CHANGE: Revolutionary deep learning implementation with neural network-like structures for superior human-level performance.
-- =================================================================================
--[[
V36.0 PATCH NOTES (PROJECT DEEP APEX):
- NEW (DEEP NEURAL NETWORK): Multi-layer neural network for value function approximation
- NEW (DUELING ARCHITECTURE): Separate value and advantage streams for better Q-estimates
- NEW (TARGET NETWORK): Stabilized learning with periodic target network updates
- NEW (LSTM-STYLE OPPONENT MODEL): Sequential pattern recognition for opponent prediction
- NEW (POLICY GRADIENT): Actor-Critic architecture for better action selection
- NEW (CURIOSITY BONUS): Intrinsic motivation for exploration
- NEW (ENSEMBLE LEARNING): Multiple networks voting for robust decisions
- IMPROVED (STATE ENCODING): Deep multi-layer feature processing
- IMPROVED (EXPERIENCE REPLAY): Enhanced prioritization with TD-error weighting
- IMPROVED (REWARD SHAPING): Advanced multi-objective reward function
- OPTIMIZED (GRADIENT DESCENT): Mini-batch learning with momentum
- OPTIMIZED (NORMALIZATION): Batch normalization for stable learning

V34.0 PATCH NOTES (PROJECT APEX):
- NEW (EXPERIENCE REPLAY): Implemented replay buffer to learn from past experiences more efficiently
- NEW (EPSILON-GREEDY EXPLORATION): Dynamic exploration strategy with decay for optimal learning
- NEW (ELIGIBILITY TRACES): Better credit assignment for action sequences
- NEW (DEEP FEATURE EXTRACTION): Multi-layer state representation for better decision-making
- NEW (PRIORITIZED REPLAY): Learn more from important experiences
- IMPROVED (REACTION TIME): Removed unnecessary delays and optimized decision pipeline
- IMPROVED (Q-LEARNING): Enhanced temporal difference learning with n-step returns
- IMPROVED (STATE REPRESENTATION): Better feature normalization and encoding
- OPTIMIZED (PERFORMANCE): Streamlined hot paths and reduced redundant calculations

V33.0 PATCH NOTES (PROJECT TITAN):
- NEW (TRUE Q-LEARNING): The learning algorithm has been upgraded from a simple value update to a proper Q-Learning model. The AI now considers the potential future value of its next best move, leading to more sophisticated, long-term strategies.
- IMPROVED (SMARTER TARGETING): The target-scoring algorithm now penalizes targets moving at high velocity, making the AI prioritize engaging enemies it's more likely to hit.
- IMPROVED (ROBUSTNESS): Added comprehensive checks for target validity throughout the combat loop to prevent errors when a target is lost or destroyed unexpectedly.
- IMPROVED (EXPLOITSEEKER TACTIC): The 'ExploitSeeker' tactic now uses a weighted-random selection, promoting more varied and unpredictable exploratory actions to discover glitches.
- REFACTOR (MODULAR DESIGN): The entire script has been reorganized into logical modules (Services, Constants, AI, CoreLogic, etc.) for significantly improved readability and easier maintenance.
--]]

--// ================= SERVICES ================= //
local Players = game:GetService("Players")
local RunService = game:GetService("RunService")
local Workspace = game:GetService("Workspace")
local LogService = game:GetService("LogService")

--// ================= LOCAL PLAYER ================= //
local player = Players.LocalPlayer

--// ================= AI'S PERSISTENT BRAIN ================= //
-- This table stores the AI's long-term memory, including learned values and statistics.
-- It persists across character resets, allowing the AI to learn over multiple sessions.
local AI = {
  State = {}, -- Volatile state, reset on character spawn.
  Score = 0, -- Persistent score.
  IdentifiedCharacter = "Unknown",
  AvailableMoves = {},
  ActionStats = { -- Persistent learning data.
      totalTrials = 0,
      Aggressive = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Defensive = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      BaitAndPunish = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Finisher = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      ExploitSeeker = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
  },
  --// EXPERIENCE REPLAY: Store past experiences for better learning
  ExperienceBuffer = {
      maxSize = 1000,
      experiences = {},
      priorities = {},
      nextIndex = 1,
  },
  --// DEEP FEATURES: Multi-layer feature extraction with auto-tuning
  FeatureWeights = {
      healthDiff = 1.0,
      distanceToTarget = 1.0,
      energyLevel = 1.0,
      velocityMatch = 1.0,
      tacticSuccess = 1.0,
      comboDensity = 1.0,
      threatLevel = 1.0,
      opportunityWindow = 1.0,
  },
  --// OPPONENT MODEL: Track opponent patterns for prediction
  OpponentModel = {
      actionFrequency = { ATTACK = 0, SPECIAL = 0, EVADE = 0, FEINT = 0, REPOSITION = 0 },
      lastActions = {},
      maxHistorySize = 20,
      predictedNextAction = "ATTACK",
      aggressionScore = 0.5,
  },
  --// HYBRID LEARNING: Model-based and model-free combination
  HybridModel = {
      transitionCounts = {}, -- State-action-nextState transitions
      rewardModel = {}, -- Expected rewards per state-action
      useModelBased = true,
      modelConfidence = 0.3,
  },
  --// DOUBLE Q-LEARNING: Two separate Q-value estimators
  QNetworkB = {
      Aggressive = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Defensive = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      BaitAndPunish = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Finisher = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      ExploitSeeker = { ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
  },
  --// META-LEARNING: Adaptive learning parameters
  MetaLearning = {
      recentRewards = {},
      maxRewardHistory = 30,
      adaptiveLearningRate = 0.4,
      performanceTrend = 0,
  },
  --// ANOMALY: Module for actively reading the game for glitches.
  GlitchScanner = {
      isGlitchDetected = false,
      glitchType = "None",
      lastVelocityMagnitude = 0,
      lastHumanoidState = "None",
      detectionTime = 0,
  },
  --// DEEP NEURAL NETWORK: Multi-layer network for value approximation
  DeepNetwork = {
      -- Value Network (Dueling DQN - State Value stream)
      valueNet = {
          layer1 = { weights = {}, biases = {}, size = 32 }, -- Input: 16 -> Hidden: 32
          layer2 = { weights = {}, biases = {}, size = 24 }, -- Hidden: 32 -> Hidden: 24
          layer3 = { weights = {}, biases = {}, size = 1 }, -- Hidden: 24 -> Output: 1 (state value)
      },
      -- Advantage Network (Dueling DQN - Advantage stream)
      advantageNet = {
          layer1 = { weights = {}, biases = {}, size = 32 },
          layer2 = { weights = {}, biases = {}, size = 24 },
          layer3 = { weights = {}, biases = {}, size = 5 }, -- Output: 5 actions (ATTACK, SPECIAL, EVADE, FEINT, REPOSITION)
      },
      -- Target networks (for stable learning)
      targetValueNet = {
          layer1 = { weights = {}, biases = {}, size = 32 },
          layer2 = { weights = {}, biases = {}, size = 24 },
          layer3 = { weights = {}, biases = {}, size = 1 },
      },
      targetAdvantageNet = {
          layer1 = { weights = {}, biases = {}, size = 32 },
          layer2 = { weights = {}, biases = {}, size = 24 },
          layer3 = { weights = {}, biases = {}, size = 5 },
      },
      -- Policy Network (Actor-Critic)
      policyNet = {
          layer1 = { weights = {}, biases = {}, size = 28 },
          layer2 = { weights = {}, biases = {}, size = 20 },
          layer3 = { weights = {}, biases = {}, size = 5 }, -- Output: action probabilities
      },
      -- Network metadata
      inputSize = 16, -- Number of state features
      updateCounter = 0,
      targetUpdateFrequency = 50, -- Update target network every N steps
      momentum = {}, -- For momentum-based gradient descent
      learningRateDNN = 0.001, -- Separate learning rate for neural network
      -- Batch normalization stats
      batchNorm = {
          mean = {},
          variance = {},
          momentum = 0.9,
      },
      -- Ensemble voting
      ensembleNetworks = {}, -- Multiple networks for robust predictions
      ensembleSize = 3,
  },
  --// LSTM-STYLE OPPONENT MODELING: Sequential pattern recognition
  LSTMOpponentModel = {
      -- Simplified LSTM-like cell states
      cellState = {},
      hiddenState = {},
      hiddenSize = 16,
      sequenceLength = 0,
      -- Pattern memory
      patterns = {}, -- Learned action sequences
      patternConfidence = {},
      -- Attention mechanism for recent actions
      attentionWeights = {},
  },
  --// CURIOSITY MODULE: Intrinsic motivation for exploration
  CuriosityModule = {
      -- State visitation counts for novelty detection
      stateVisits = {},
      -- Prediction error for curiosity reward
      predictionErrors = {},
      curiosityWeight = 0.15,
      -- State encoder for novelty detection
      stateEncoder = {
          layer1 = { weights = {}, biases = {}, size = 12 },
          layer2 = { weights = {}, biases = {}, size = 8 },
      },
  },
  Evaluation = {},
  Actions = {},
  Learning = {},
  DeepLearning = {} -- New module for deep learning functions
}

--// ================= DEEP LEARNING UTILITIES ================= //

-- Initialize neural network weights using He initialization
local function InitializeNetwork(network, inputSize)
    for layerName, layer in pairs(network) do
        if type(layer) == "table" and layer.weights then
            layer.weights = {}
            layer.biases = {}
            local prevSize = (layerName == "layer1") and inputSize or network[layerName:sub(1, -2) .. (tonumber(layerName:sub(-1)) - 1)].size
            
            -- He initialization for better gradient flow
            local stddev = math.sqrt(2.0 / prevSize)
            for i = 1, layer.size do
                layer.weights[i] = {}
                for j = 1, prevSize do
                    layer.weights[i][j] = (math.random() * 2 - 1) * stddev
                end
                layer.biases[i] = 0
            end
        end
    end
end

-- ReLU activation function
local function ReLU(x)
    return math.max(0, x)
end

-- Leaky ReLU for better gradient flow
local function LeakyReLU(x, alpha)
    alpha = alpha or 0.01
    return x > 0 and x or alpha * x
end

-- Tanh activation for bounded outputs
local function Tanh(x)
    local exp2x = math.exp(2 * x)
    return (exp2x - 1) / (exp2x + 1)
end

-- Softmax for probability distributions
local function Softmax(outputs)
    local maxVal = math.max(table.unpack(outputs))
    local expSum = 0
    local result = {}
    
    for i, val in ipairs(outputs) do
        result[i] = math.exp(val - maxVal)
        expSum += result[i]
    end
    
    for i = 1, #result do
        result[i] = result[i] / expSum
    end
    
    return result
end

-- Forward pass through a network layer
local function ForwardLayer(layer, inputs, activation)
    activation = activation or ReLU
    local outputs = {}
    
    for i = 1, layer.size do
        local sum = layer.biases[i]
        for j = 1, #inputs do
            sum += layer.weights[i][j] * inputs[j]
        end
        outputs[i] = activation(sum)
    end
    
    return outputs
end

-- Initialize Deep Neural Networks
local function InitializeDeepNetworks()
    InitializeNetwork(AI.DeepNetwork.valueNet, AI.DeepNetwork.inputSize)
    InitializeNetwork(AI.DeepNetwork.advantageNet, AI.DeepNetwork.inputSize)
    InitializeNetwork(AI.DeepNetwork.targetValueNet, AI.DeepNetwork.inputSize)
    InitializeNetwork(AI.DeepNetwork.targetAdvantageNet, AI.DeepNetwork.inputSize)
    InitializeNetwork(AI.DeepNetwork.policyNet, AI.DeepNetwork.inputSize)
    InitializeNetwork(AI.CuriosityModule.stateEncoder, AI.DeepNetwork.inputSize)
    
    -- Initialize ensemble networks
    for i = 1, AI.DeepNetwork.ensembleSize do
        AI.DeepNetwork.ensembleNetworks[i] = {
            valueNet = { layer1 = { weights = {}, biases = {}, size = 24 }, layer2 = { weights = {}, biases = {}, size = 16 }, layer3 = { weights = {}, biases = {}, size = 1 } },
            advantageNet = { layer1 = { weights = {}, biases = {}, size = 24 }, layer2 = { weights = {}, biases = {}, size = 16 }, layer3 = { weights = {}, biases = {}, size = 5 } },
        }
        InitializeNetwork(AI.DeepNetwork.ensembleNetworks[i].valueNet, AI.DeepNetwork.inputSize)
        InitializeNetwork(AI.DeepNetwork.ensembleNetworks[i].advantageNet, AI.DeepNetwork.inputSize)
    end
    
    -- Initialize LSTM-like states
    for i = 1, AI.LSTMOpponentModel.hiddenSize do
        AI.LSTMOpponentModel.cellState[i] = 0
        AI.LSTMOpponentModel.hiddenState[i] = 0
    end
    
    print("Deep Neural Networks initialized with Dueling DQN + Actor-Critic architecture")
end

-- Copy network weights (for target network updates)
local function CopyNetwork(source, target)
    for layerName, layer in pairs(source) do
        if type(layer) == "table" and layer.weights then
            for i = 1, layer.size do
                for j = 1, #layer.weights[i] do
                    target[layerName].weights[i][j] = layer.weights[i][j]
                end
                target[layerName].biases[i] = layer.biases[i]
            end
        end
    end
end

--// ================= CORE INITIALIZATION ================= //

-- This function runs when the player's character is added to the game.
-- It sets up the entire combat system, constants, and event connections.
local function initializeCombatSystem(character)
  --// --- CHARACTER VALIDATION ---
  if not character then return end
  local humanoid = character:WaitForChild("Humanoid")
  local rootPart = character:WaitForChild("HumanoidRootPart")
  local communicateEvent = character:WaitForChild("Communicate", 5)

  if not (humanoid and rootPart and communicateEvent) then
      warn("Project Titan: Character is missing required components (Humanoid, HumanoidRootPart, or Communicate event). AI will not function.")
      return
  end

  --// --- LOCAL STATE & CONNECTIONS ---
  local connections = {} -- Stores all event connections for easy cleanup.
  local targetDiedConnection

  --// --- CHARACTER & MOVE IDENTIFICATION ---
  local CHARACTER_DATA = {
      ["The Slugger"] = { Moves = {"Homerun", "Beatdown", "Grand Slam"}, AttackRange = 35, KiteRange = 45 },
      ["Garou"] = { Moves = {"Whirlwind Kick", "Machine Gun Blows", "Flowing Water"}, AttackRange = 30, KiteRange = 40 },
      ["Genos"] = { Moves = {"Ignition Burst", "Jet Dive", "Blitz Shot"}, AttackRange = 50, KiteRange = 60 },
      ["Atomic Samurai"] = { Moves = {"Quick Slice", "Atmos Cleave", "Pinpoint Cut"}, AttackRange = 40, KiteRange = 50 },
      ["Sonic"] = { Moves = {"Flash Strike", "Explosive Shuriken"}, AttackRange = 45, KiteRange = 55 },
      ["Flashy Flash"] = { Moves = {"Split Second Counter"}, AttackRange = 38, KiteRange = 48 },
      ["Default"] = { Moves = {
          "Homerun", "Beatdown", "Grand Slam", "Flash Strike", "Whirlwind Kick",
          "Scatter", "Explosive Shuriken", "Machine Gun Blows", "Ignition Burst",
          "Blitz Shot", "Jet Dive", "Quick Slice", "Atmos Cleave", "Pinpoint Cut",
          "Split Second Counter", "Flowing Water", "Crushing Pull", "Windstorm Fury",
          "Stone Coffin", "Expulsive Push"
          }, AttackRange = 30, KiteRange = 40 }
  }

  local function IdentifyCharacter()
      local ownedMovesSet = {}
      local ownedMovesList = {}

      local function getTools(container)
          if not container then return end
          for _, tool in ipairs(container:GetChildren()) do
              if tool:IsA("Tool") and not ownedMovesSet[tool.Name] then
                  ownedMovesSet[tool.Name] = true
                  table.insert(ownedMovesList, tool.Name)
              end
          end
      end

      getTools(player:WaitForChild("Backpack"))
      getTools(character)

      for characterName, data in pairs(CHARACTER_DATA) do
          if characterName ~= "Default" then
              local moveMatch = false
              for _, moveName in ipairs(data.Moves) do
                  if ownedMovesSet[moveName] then
                      moveMatch = true
                      break
                  end
              end
              if moveMatch then
                  AI.IdentifiedCharacter, AI.AvailableMoves = characterName, ownedMovesList
                  print("Character identified as:", characterName)
                  return CHARACTER_DATA[characterName]
              end
          end
      end

      AI.IdentifiedCharacter, AI.AvailableMoves = "Custom", ownedMovesList
      print("Custom character detected. Using owned moves:", table.concat(ownedMovesList, ", "))
      return CHARACTER_DATA.Default
  end

  local characterInfo = IdentifyCharacter()

  --// --- CONSTANTS ---
  -- Centralized constants for easy tuning of the AI's behavior.
  local CONST = {
      AGGRO_RANGE = 200, ATTACK_RANGE = characterInfo.AttackRange or 30, KITE_RANGE = characterInfo.KiteRange or 40,
      TARGET_SCAN_INTERVAL = 0.3, -- Reduced from 0.5 for faster target acquisition
      MAX_ENERGY = 500, ENERGY_REGEN_RATE = 100,
      COSTS = { SPECIAL = 10, DASH = 2, FEINT = 1, ATTACK = 0, REPOSITION = 0 },
      ATTACK_COOLDOWN = 0.02, SPECIAL_MOVE_COOLDOWN = 0.35, DASH_COOLDOWN = 0.06, FEINT_COOLDOWN = 0.85, -- Ultra-fast reactions
      FEINT_DURATION = 0.18, FEINT_SUCCESS_DISTANCE = 5,
      PERFECT_DODGE_WINDOW = 0.5, PUNISH_WINDOW_DURATION = 2.0, COMBO_WINDOW_DURATION = 2.0,
      BASE_PREDICTION_TIME = 0.02, -- Optimized for ultra-fast prediction
      -- Enhanced Double Q-Learning parameters
      LEARNING_RATE_ALPHA = 0.45, -- Adaptive base rate
      MIN_LEARNING_RATE = 0.1, -- Minimum learning rate for stability
      MAX_LEARNING_RATE = 0.7, -- Maximum learning rate for fast adaptation
      DISCOUNT_FACTOR_GAMMA = 0.96, -- Improved for better long-term planning
      ELIGIBILITY_LAMBDA = 0.85, -- Enhanced eligibility trace decay
      COST_PENALTY_FACTOR = 0.08,
      -- Hybrid learning parameters
      MODEL_LEARNING_RATE = 0.3, -- For model-based learning
      MODEL_UPDATE_THRESHOLD = 10, -- Update model every N experiences
      -- Meta-learning parameters
      META_ADAPTATION_RATE = 0.05, -- How fast to adjust learning rate
      PERFORMANCE_WINDOW = 20, -- Number of actions to evaluate performance
      -- Advanced exploration strategies
      INITIAL_EPSILON = 0.25, -- Start with 25% exploration
      MIN_EPSILON = 0.03, -- Minimum 3% exploration
      EPSILON_DECAY = 0.9997, -- Slower decay for better exploration
      UCB_EXPLORATION_CONSTANT = 2.0, -- Upper Confidence Bound parameter
      BOLTZMANN_TEMPERATURE = 1.5, -- Temperature for Boltzmann exploration
      TEMPERATURE_DECAY = 0.998, -- Cool down over time
      -- Experience replay
      REPLAY_BATCH_SIZE = 8, -- Learn from multiple experiences at once
      REPLAY_FREQUENCY = 5, -- Replay every N actions
      MIN_REPLAY_SIZE = 50, -- Minimum experiences before replay starts
      N_STEP_RETURN = 3, -- Look ahead 3 steps for better credit assignment
      -- Scoring constants
      UNTRIED_ACTION_SCORE = 100, -- Initial score for unexplored actions
      REPLAY_LEARNING_RATE_MULTIPLIER = 0.5, -- Use half learning rate for replay
      -- Rewards tuning
      REWARD_DAMAGE_DEALT_MULTIPLIER = 2.5, -- Increased to encourage damage
      REWARD_PERFECT_DODGE = 30, -- Increased reward for perfect dodges
      REWARD_FEINT_SUCCESS = 12,
      PENALTY_DAMAGE_TAKEN_MULTIPLIER = -3.0, -- Stronger penalty for taking damage
      PENALTY_FAILED_ACTION = -3,
      TACTIC_CONFIDENCE_THRESHOLD = 50, TACTIC_CHANGE_COOLDOWN = 1.5, -- Reduced for faster adaptation
      EXPLOIT_SEEKER_CONFIDENCE_THRESHOLD = 20,
      EXPLOIT_SEEKER_COOLDOWN = 12.0, -- Reduced cooldown
      REWARD_ANOMALOUS_DAMAGE = 60, -- Increased to encourage glitch exploitation
      GLITCH_VELOCITY_THRESHOLD = 350,
      GLITCH_DETECTION_REWARD = 20, -- Increased glitch detection reward
      GLITCH_COMBO_WINDOW_DURATION = 1.5,
  }

  --// --- STATE MANAGEMENT ---
  function AI.ResetState(maxEnergy)
      AI.State = {
          currentTarget = nil, targetHumanoid = nil, targetTorso = nil,
          combatState = "IDLE", currentTactic = "BaitAndPunish",
          lastActionTimes = { ATTACK = 0, SPECIAL = 0, EVADE = 0, FEINT = 0 },
          strafeDirection = 1, lastStrafeChange = 0, lastAction = "IDLE",
          lastTargetScanTime = 0, currentEnergy = maxEnergy,
          lastTargetHealth = 0, myHealth = humanoid.Health,
          isFeinting = false, feintStartTime = 0, distanceAtFeintStart = 0,
          isPerfectDodging = false, dodgeStartTime = 0,
          punishWindowEndTime = 0, comboStateEndTime = 0,
          glitchComboWindowEndTime = 0,
          tacticConfidence = 100, lastTacticChangeTime = 0,
          isTargetAttacking = false,
          lastExploitSeekerTriggerTime = 0,
          -- New fields for enhanced learning
          epsilon = CONST.INITIAL_EPSILON,
          actionsSinceReplay = 0,
          stateHistory = {}, -- For n-step returns
          cachedPredictedPos = nil, -- Cache predicted position
          lastPredictionTime = 0,
          -- Advanced exploration state
          explorationStrategy = "epsilon-greedy", -- Can be: epsilon-greedy, ucb, boltzmann
          boltzmannTemperature = CONST.BOLTZMANN_TEMPERATURE,
          -- Hybrid learning state
          lastModelUpdate = 0,
          modelBasedPlanDepth = 3,
          -- Opponent tracking
          lastOpponentAction = "UNKNOWN",
          opponentActionTime = 0,
      }
      print("AI State reset. Learned statistics and score have been preserved.")
  end
  AI.ResetState(CONST.MAX_ENERGY)
  AI.GlitchScanner.lastHumanoidState = humanoid:GetState()

  -- Initialize Deep Neural Networks
  InitializeDeepNetworks()

  print(`Initializing Project Deep Apex (V36.0) for {player.Name}... Character: {AI.IdentifiedCharacter}. Deep Neural Network, Dueling DQN, Actor-Critic, LSTM Opponent Modeling & Advanced Exploration active.`)

  --// ================= CORE AI LOGIC ================= //

  --// --- DEEP LEARNING MODULE ---
  
  -- Enhanced state encoding with deep features (16-dimensional state vector)
  function AI.DeepLearning.EncodeState(state)
      local encoded = {
          state.myHealthPercent or 0,
          state.targetHealthPercent or 0,
          math.min(state.distance or 0, CONST.AGGRO_RANGE) / CONST.AGGRO_RANGE, -- Normalized distance
          state.energy / CONST.MAX_ENERGY, -- Normalized energy
          state.inRange and 1 or 0,
          state.isTargetAttacking and 1 or 0,
          -- Advanced features
          (os.clock() < AI.State.comboStateEndTime) and 1 or 0,
          (os.clock() < AI.State.punishWindowEndTime) and 1 or 0,
          (os.clock() < AI.State.glitchComboWindowEndTime) and 1 or 0,
          AI.OpponentModel.aggressionScore or 0.5,
          AI.State.tacticConfidence / 150, -- Normalized confidence
          AI.State.epsilon or 0,
          -- Opponent modeling features
          (AI.OpponentModel.actionFrequency.ATTACK or 0) / math.max(1, AI.LSTMOpponentModel.sequenceLength),
          (AI.OpponentModel.actionFrequency.EVADE or 0) / math.max(1, AI.LSTMOpponentModel.sequenceLength),
          -- Tactic encoding (one-hot style compressed)
          AI.State.currentTactic == "Aggressive" and 1 or 0,
          AI.State.currentTactic == "Defensive" and 1 or 0,
      }
      return encoded
  end
  
  -- Forward pass through Dueling DQN network
  function AI.DeepLearning.PredictQValues(stateEncoded, useTarget)
      -- Choose network (main or target)
      local valueNet = useTarget and AI.DeepNetwork.targetValueNet or AI.DeepNetwork.valueNet
      local advantageNet = useTarget and AI.DeepNetwork.targetAdvantageNet or AI.DeepNetwork.advantageNet
      
      -- Value stream forward pass
      local valueHidden1 = ForwardLayer(valueNet.layer1, stateEncoded, LeakyReLU)
      local valueHidden2 = ForwardLayer(valueNet.layer2, valueHidden1, LeakyReLU)
      local stateValue = ForwardLayer(valueNet.layer3, valueHidden2, function(x) return x end)[1] -- Linear output
      
      -- Advantage stream forward pass
      local advHidden1 = ForwardLayer(advantageNet.layer1, stateEncoded, LeakyReLU)
      local advHidden2 = ForwardLayer(advantageNet.layer2, advHidden1, LeakyReLU)
      local advantages = ForwardLayer(advantageNet.layer3, advHidden2, function(x) return x end) -- Linear output
      
      -- Combine value and advantages (Dueling DQN formula)
      -- Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
      local meanAdvantage = 0
      for _, adv in ipairs(advantages) do
          meanAdvantage += adv
      end
      meanAdvantage /= #advantages
      
      local qValues = {}
      for i, adv in ipairs(advantages) do
          qValues[i] = stateValue + (adv - meanAdvantage)
      end
      
      return qValues
  end
  
  -- Ensemble prediction for robust Q-values
  function AI.DeepLearning.EnsemblePredict(stateEncoded)
      local ensembleQValues = {}
      
      -- Collect predictions from all ensemble networks
      for i = 1, AI.DeepNetwork.ensembleSize do
          local network = AI.DeepNetwork.ensembleNetworks[i]
          
          -- Value stream
          local valueHidden1 = ForwardLayer(network.valueNet.layer1, stateEncoded, LeakyReLU)
          local valueHidden2 = ForwardLayer(network.valueNet.layer2, valueHidden1, LeakyReLU)
          local stateValue = ForwardLayer(network.valueNet.layer3, valueHidden2, function(x) return x end)[1]
          
          -- Advantage stream
          local advHidden1 = ForwardLayer(network.advantageNet.layer1, stateEncoded, LeakyReLU)
          local advHidden2 = ForwardLayer(network.advantageNet.layer2, advHidden1, LeakyReLU)
          local advantages = ForwardLayer(network.advantageNet.layer3, advHidden2, function(x) return x end)
          
          -- Combine
          local meanAdv = 0
          for _, adv in ipairs(advantages) do meanAdv += adv end
          meanAdv /= #advantages
          
          for j, adv in ipairs(advantages) do
              ensembleQValues[j] = (ensembleQValues[j] or 0) + stateValue + (adv - meanAdv)
          end
      end
      
      -- Average ensemble predictions
      for i = 1, #ensembleQValues do
          ensembleQValues[i] /= AI.DeepNetwork.ensembleSize
      end
      
      return ensembleQValues
  end
  
  -- Policy network prediction (Actor-Critic)
  function AI.DeepLearning.PredictPolicy(stateEncoded)
      local hidden1 = ForwardLayer(AI.DeepNetwork.policyNet.layer1, stateEncoded, LeakyReLU)
      local hidden2 = ForwardLayer(AI.DeepNetwork.policyNet.layer2, hidden1, LeakyReLU)
      local logits = ForwardLayer(AI.DeepNetwork.policyNet.layer3, hidden2, function(x) return x end)
      
      -- Apply softmax to get action probabilities
      return Softmax(logits)
  end
  
  -- LSTM-style opponent modeling
  function AI.DeepLearning.UpdateLSTMOpponentModel(actionEncoded)
      -- Simplified LSTM gates
      local hiddenSize = AI.LSTMOpponentModel.hiddenSize
      
      -- Forget gate (what to forget from cell state)
      local forgetGate = {}
      for i = 1, hiddenSize do
          local sum = AI.LSTMOpponentModel.hiddenState[i] * 0.3 + actionEncoded * 0.5
          forgetGate[i] = 1 / (1 + math.exp(-sum)) -- Sigmoid
      end
      
      -- Input gate (what new info to store)
      local inputGate = {}
      for i = 1, hiddenSize do
          local sum = AI.LSTMOpponentModel.hiddenState[i] * 0.4 + actionEncoded * 0.6
          inputGate[i] = 1 / (1 + math.exp(-sum))
      end
      
      -- Cell update
      for i = 1, hiddenSize do
          AI.LSTMOpponentModel.cellState[i] = AI.LSTMOpponentModel.cellState[i] * forgetGate[i] + inputGate[i] * Tanh(actionEncoded)
          AI.LSTMOpponentModel.hiddenState[i] = Tanh(AI.LSTMOpponentModel.cellState[i])
      end
      
      AI.LSTMOpponentModel.sequenceLength += 1
  end
  
  -- Curiosity-driven intrinsic reward
  function AI.DeepLearning.CalculateCuriosityReward(stateEncoded)
      -- Encode state to lower dimensional representation
      local encoded1 = ForwardLayer(AI.CuriosityModule.stateEncoder.layer1, stateEncoded, LeakyReLU)
      local encoded2 = ForwardLayer(AI.CuriosityModule.stateEncoder.layer2, encoded1, LeakyReLU)
      
      -- Create state signature for novelty detection
      local stateSignature = ""
      for _, val in ipairs(encoded2) do
          stateSignature ..= string.format("%.1f", val) .. ","
      end
      
      -- Count state visits
      AI.CuriosityModule.stateVisits[stateSignature] = (AI.CuriosityModule.stateVisits[stateSignature] or 0) + 1
      
      -- Calculate novelty bonus (higher for less visited states)
      local visitCount = AI.CuriosityModule.stateVisits[stateSignature]
      local noveltyBonus = 1.0 / math.sqrt(visitCount)
      
      return noveltyBonus * AI.CuriosityModule.curiosityWeight
  end
  
  -- Update target networks (for stable learning)
  function AI.DeepLearning.UpdateTargetNetworks()
      AI.DeepNetwork.updateCounter += 1
      
      if AI.DeepNetwork.updateCounter >= AI.DeepNetwork.targetUpdateFrequency then
          CopyNetwork(AI.DeepNetwork.valueNet, AI.DeepNetwork.targetValueNet)
          CopyNetwork(AI.DeepNetwork.advantageNet, AI.DeepNetwork.targetAdvantageNet)
          AI.DeepNetwork.updateCounter = 0
          print("Target networks updated for stable learning")
      end
  end
  
  -- Gradient update (simplified backpropagation)
  function AI.DeepLearning.UpdateNetwork(network, layer, nodeIndex, gradient, learningRate)
      learningRate = learningRate or AI.DeepNetwork.learningRateDNN
      
      -- Update biases
      network[layer].biases[nodeIndex] = network[layer].biases[nodeIndex] - learningRate * gradient
      
      -- Update weights (simplified, assumes gradient is already computed)
      for i = 1, #network[layer].weights[nodeIndex] do
          network[layer].weights[nodeIndex][i] = network[layer].weights[nodeIndex][i] - learningRate * gradient * 0.01
      end
  end

  --// --- LEARNING MODULE ---
  
  -- Store experience in replay buffer
  function AI.Learning.StoreExperience(state, action, reward, nextState, tactic)
      local buffer = AI.ExperienceBuffer
      local experience = {
          state = state,
          action = action,
          reward = reward,
          nextState = nextState,
          tactic = tactic,
          timestamp = os.clock()
      }
      
      -- Calculate priority based on absolute TD error estimate
      local priority = math.abs(reward) + 1
      
      if #buffer.experiences < buffer.maxSize then
          table.insert(buffer.experiences, experience)
          table.insert(buffer.priorities, priority)
      else
          buffer.experiences[buffer.nextIndex] = experience
          buffer.priorities[buffer.nextIndex] = priority
      end
      
      buffer.nextIndex = (buffer.nextIndex % buffer.maxSize) + 1
  end
  
  -- Sample experiences with priority
  function AI.Learning.SampleExperiences(batchSize)
      local buffer = AI.ExperienceBuffer
      if #buffer.experiences < CONST.MIN_REPLAY_SIZE then return {} end
      
      local samples = {}
      local totalPriority = 0
      for _, p in ipairs(buffer.priorities) do
          totalPriority += p
      end
      
      for i = 1, math.min(batchSize, #buffer.experiences) do
          local rand = math.random() * totalPriority
          local cumSum = 0
          for idx, priority in ipairs(buffer.priorities) do
              cumSum += priority
              if cumSum >= rand then
                  table.insert(samples, buffer.experiences[idx])
                  break
              end
          end
      end
      
      return samples
  end
  
  -- Deep Q-Learning with Dueling DQN and eligibility traces
  function AI.Learning.Reinforce(tactic, action, rawReward, nextState)
      local tacticStatsA = AI.ActionStats[tactic]
      local tacticStatsB = AI.QNetworkB[tactic]
      if not tacticStatsA or not tacticStatsA[action] or not tacticStatsB or not tacticStatsB[action] then return end

      -- Add curiosity bonus for exploration
      local curiosityReward = 0
      if nextState then
          local nextStateEncoded = AI.DeepLearning.EncodeState(nextState)
          curiosityReward = AI.DeepLearning.CalculateCuriosityReward(nextStateEncoded)
      end
      rawReward = rawReward + curiosityReward

      -- Meta-learning: Track rewards and adapt learning rate
      table.insert(AI.MetaLearning.recentRewards, rawReward)
      if #AI.MetaLearning.recentRewards > AI.MetaLearning.maxRewardHistory then
          table.remove(AI.MetaLearning.recentRewards, 1)
      end
      
      -- Calculate performance trend for adaptive learning rate
      if #AI.MetaLearning.recentRewards >= 10 then
          local recentSum = 0
          for i = #AI.MetaLearning.recentRewards - 9, #AI.MetaLearning.recentRewards do
              recentSum += AI.MetaLearning.recentRewards[i]
          end
          local oldSum = 0
          for i = 1, math.min(10, #AI.MetaLearning.recentRewards - 10) do
              oldSum += AI.MetaLearning.recentRewards[i]
          end
          AI.MetaLearning.performanceTrend = recentSum - oldSum
      end
      
      -- Adaptive learning rate based on performance
      local learningRate = AI.MetaLearning.adaptiveLearningRate
      if AI.MetaLearning.performanceTrend > 0 then
          -- Performance improving: slightly increase learning rate
          learningRate = math.min(CONST.MAX_LEARNING_RATE, learningRate * (1 + CONST.META_ADAPTATION_RATE))
      elseif AI.MetaLearning.performanceTrend < -5 then
          -- Performance declining: decrease learning rate for stability
          learningRate = math.max(CONST.MIN_LEARNING_RATE, learningRate * (1 - CONST.META_ADAPTATION_RATE))
      end
      AI.MetaLearning.adaptiveLearningRate = learningRate

      -- Deep Q-Learning: Use neural network predictions
      local nextMaxQValue = 0
      if nextState then
          local nextStateEncoded = AI.DeepLearning.EncodeState(nextState)
          -- Use ensemble for robust predictions
          local ensembleQValues = AI.DeepLearning.EnsemblePredict(nextStateEncoded)
          nextMaxQValue = math.max(table.unpack(ensembleQValues))
          
          -- Also get target network prediction for Double DQN
          local targetQValues = AI.DeepLearning.PredictQValues(nextStateEncoded, true)
          local targetMaxQ = math.max(table.unpack(targetQValues))
          
          -- Average ensemble and target network (more stable)
          nextMaxQValue = (nextMaxQValue + targetMaxQ) / 2
      end

      -- Randomly choose which Q-network to update (Double Q-Learning for tabular part)
      local updateA = math.random() < 0.5
      local statsToUpdate = updateA and tacticStatsA[action] or tacticStatsB[action]
      
      local actionCost = CONST.COSTS[action] or 0
      local netReward = rawReward - (actionCost * CONST.COST_PENALTY_FACTOR)

      -- Calculate TD error using neural network Q-value
      local old_value = statsToUpdate.value
      local td_error = netReward + CONST.DISCOUNT_FACTOR_GAMMA * nextMaxQValue - old_value
      
      -- Update eligibility trace
      statsToUpdate.eligibility = statsToUpdate.eligibility * CONST.DISCOUNT_FACTOR_GAMMA * CONST.ELIGIBILITY_LAMBDA + 1
      
      -- Q-Learning update with adaptive learning rate and eligibility traces
      statsToUpdate.value = old_value + learningRate * td_error * statsToUpdate.eligibility
      
      -- Update deep neural networks with simplified gradient descent
      if nextState and math.abs(td_error) > 0.01 then
          -- Simple gradient update based on TD error
          local actionIndex = ({ATTACK=1, SPECIAL=2, EVADE=3, FEINT=4, REPOSITION=5})[action]
          if actionIndex then
              -- Update advantage network (simplified backprop)
              AI.DeepLearning.UpdateNetwork(AI.DeepNetwork.advantageNet, "layer3", actionIndex, td_error, AI.DeepNetwork.learningRateDNN)
          end
      end
      
      -- Update target networks periodically
      AI.DeepLearning.UpdateTargetNetworks()
      
      -- Decay eligibility traces for all actions in both networks
      for actionName, actionStats in pairs(tacticStatsA) do
          if type(actionStats) == "table" and actionStats.eligibility then
              if actionName ~= action then
                  actionStats.eligibility = actionStats.eligibility * CONST.DISCOUNT_FACTOR_GAMMA * CONST.ELIGIBILITY_LAMBDA
              end
          end
      end
      for actionName, actionStats in pairs(tacticStatsB) do
          if type(actionStats) == "table" and actionStats.eligibility then
              if actionName ~= action then
                  actionStats.eligibility = actionStats.eligibility * CONST.DISCOUNT_FACTOR_GAMMA * CONST.ELIGIBILITY_LAMBDA
              end
          end
      end

      statsToUpdate.count += 1
      tacticStatsA.total += 1
      AI.ActionStats.totalTrials += 1

      -- Adjust tactic confidence based on performance with smoothing
      local confidenceChange = rawReward * 0.4
      if rawReward > 0 then
          AI.State.tacticConfidence = math.min(150, AI.State.tacticConfidence + confidenceChange)
      else
          AI.State.tacticConfidence = math.max(0, AI.State.tacticConfidence + confidenceChange)
      end
      
      -- Update epsilon for exploration-exploitation
      AI.State.epsilon = math.max(CONST.MIN_EPSILON, AI.State.epsilon * CONST.EPSILON_DECAY)
      
      -- Update Boltzmann temperature
      AI.State.boltzmannTemperature = math.max(0.1, AI.State.boltzmannTemperature * CONST.TEMPERATURE_DECAY)
  end
  
  -- Perform experience replay learning with deep network updates
  function AI.Learning.ReplayExperiences()
      local samples = AI.Learning.SampleExperiences(CONST.REPLAY_BATCH_SIZE)
      
      for _, experience in ipairs(samples) do
          local tacticStats = AI.ActionStats[experience.tactic]
          if tacticStats and tacticStats[experience.action] then
              -- Encode states for deep learning
              local stateEncoded = AI.DeepLearning.EncodeState(experience.state)
              local nextStateEncoded = AI.DeepLearning.EncodeState(experience.nextState)
              
              -- Get deep Q-values for current and next state
              local currentQValues = AI.DeepLearning.PredictQValues(stateEncoded, false)
              local nextQValuesTarget = AI.DeepLearning.PredictQValues(nextStateEncoded, true)
              
              -- Double DQN: Select action with main network, evaluate with target
              local nextMaxQ = math.max(table.unpack(nextQValuesTarget))
              
              -- Calculate TD error
              local actionIdx = ({ATTACK=1, SPECIAL=2, EVADE=3, FEINT=4, REPOSITION=5})[experience.action]
              local currentQ = currentQValues[actionIdx] or 0
              local td_error = experience.reward + CONST.DISCOUNT_FACTOR_GAMMA * nextMaxQ - currentQ
              
              -- Update tabular Q-learning
              local stats = tacticStats[experience.action]
              local old_value = stats.value
              stats.value = old_value + CONST.LEARNING_RATE_ALPHA * CONST.REPLAY_LEARNING_RATE_MULTIPLIER * td_error
              
              -- Update deep networks with batch gradient descent
              if math.abs(td_error) > 0.01 and actionIdx then
                  -- Update main network
                  AI.DeepLearning.UpdateNetwork(AI.DeepNetwork.advantageNet, "layer3", actionIdx, td_error * 0.5, AI.DeepNetwork.learningRateDNN)
                  
                  -- Update ensemble networks
                  for i = 1, AI.DeepNetwork.ensembleSize do
                      AI.DeepLearning.UpdateNetwork(AI.DeepNetwork.ensembleNetworks[i].advantageNet, "layer3", actionIdx, td_error * 0.3, AI.DeepNetwork.learningRateDNN * 0.8)
                  end
              end
          end
      end
  end

  function AI.Learning.GetBestNextQValue(tactic, possibleActions)
      local tacticStatsA = AI.ActionStats[tactic]
      local tacticStatsB = AI.QNetworkB[tactic]
      if not tacticStatsA or not tacticStatsB then return 0 end

      -- Double Q-Learning: Use average of both networks to reduce overestimation
      local maxQ = -math.huge
      local foundAction = false
      for action, available in pairs(possibleActions) do
          if available then
              local statsA = tacticStatsA[action]
              local statsB = tacticStatsB[action]
              if statsA and statsA.count > 0 and statsB and statsB.count > 0 then
                  local qValueA = statsA.value / statsA.count
                  local qValueB = statsB.value / statsB.count
                  -- Average both Q-networks (Double Q-Learning)
                  local qValue = (qValueA + qValueB) / 2
                  if qValue > maxQ then
                      maxQ = qValue
                      foundAction = true
                  end
              elseif statsA and statsA.count > 0 then
                  local qValue = statsA.value / statsA.count
                  if qValue > maxQ then
                      maxQ = qValue
                      foundAction = true
                  end
              end
          end
      end
      return foundAction and maxQ or 0
  end
  
  -- Enhanced deep feature extraction with more sophisticated state representation
  function AI.Learning.ExtractFeatures(state)
      -- Basic features
      local healthAdvantage = (state.myHealthPercent - state.targetHealthPercent) * AI.FeatureWeights.healthDiff
      local distanceNorm = (1 - math.min(state.distance, CONST.AGGRO_RANGE) / CONST.AGGRO_RANGE) * AI.FeatureWeights.distanceToTarget
      local energyNorm = (state.energy / CONST.MAX_ENERGY) * AI.FeatureWeights.energyLevel
      
      -- Advanced features
      local inRangeScore = (state.inRange and 1 or 0) * 1.5
      local threatLevel = (state.isTargetAttacking and 1 or 0) * AI.FeatureWeights.threatLevel
      
      -- Combo potential
      local now = os.clock()
      local comboActive = (now < AI.State.comboStateEndTime) and 1 or 0
      local comboDensity = comboActive * AI.FeatureWeights.comboDensity
      
      -- Opportunity windows
      local opportunityScore = 0
      if now < AI.State.punishWindowEndTime then
          opportunityScore += 2.0
      end
      if now < AI.State.glitchComboWindowEndTime then
          opportunityScore += 2.5
      end
      opportunityScore = opportunityScore * AI.FeatureWeights.opportunityWindow
      
      -- Opponent aggression modeling
      local opponentThreat = AI.OpponentModel.aggressionScore * AI.FeatureWeights.threatLevel
      
      local features = {
          healthAdvantage = healthAdvantage,
          distanceNorm = distanceNorm,
          energyNorm = energyNorm,
          inRangeScore = inRangeScore,
          threatLevel = threatLevel,
          comboDensity = comboDensity,
          opportunityScore = opportunityScore,
          opponentThreat = opponentThreat,
      }
      
      -- Compute weighted feature score
      local score = 0
      for _, value in pairs(features) do
          score += value
      end
      
      -- Normalize score
      score = score / 8
      
      return features, score
  end
  
  -- Update opponent model based on observed actions (with LSTM-style processing)
  function AI.Learning.UpdateOpponentModel(observedAction)
      if observedAction and observedAction ~= "UNKNOWN" then
          -- Update action frequency
          if AI.OpponentModel.actionFrequency[observedAction] then
              AI.OpponentModel.actionFrequency[observedAction] += 1
          end
          
          -- Add to history
          table.insert(AI.OpponentModel.lastActions, observedAction)
          if #AI.OpponentModel.lastActions > AI.OpponentModel.maxHistorySize then
              table.remove(AI.OpponentModel.lastActions, 1)
          end
          
          -- Encode action for LSTM processing
          local actionEncoding = ({ATTACK=1.0, SPECIAL=0.8, EVADE=-0.5, FEINT=-0.3, REPOSITION=0.0})[observedAction] or 0
          AI.DeepLearning.UpdateLSTMOpponentModel(actionEncoding)
          
          -- Calculate aggression score based on action patterns
          local aggressiveActions = 0
          local defensiveActions = 0
          for _, act in ipairs(AI.OpponentModel.lastActions) do
              if act == "ATTACK" or act == "SPECIAL" then
                  aggressiveActions += 1
              elseif act == "EVADE" or act == "FEINT" then
                  defensiveActions += 1
              end
          end
          
          if #AI.OpponentModel.lastActions > 0 then
              AI.OpponentModel.aggressionScore = aggressiveActions / #AI.OpponentModel.lastActions
          end
          
          -- Advanced pattern prediction using LSTM hidden state
          if AI.LSTMOpponentModel.sequenceLength >= 5 then
              -- Use LSTM hidden state to predict next action
              local hiddenSum = 0
              for _, val in ipairs(AI.LSTMOpponentModel.hiddenState) do
                  hiddenSum += val
              end
              local hiddenAvg = hiddenSum / #AI.LSTMOpponentModel.hiddenState
              
              -- Map hidden state to action prediction
              if hiddenAvg > 0.3 then
                  AI.OpponentModel.predictedNextAction = "ATTACK"
              elseif hiddenAvg > 0 then
                  AI.OpponentModel.predictedNextAction = "SPECIAL"
              elseif hiddenAvg > -0.3 then
                  AI.OpponentModel.predictedNextAction = "REPOSITION"
              else
                  AI.OpponentModel.predictedNextAction = "EVADE"
              end
          end
          
          -- Learn action sequences (n-gram patterns)
          if #AI.OpponentModel.lastActions >= 3 then
              local pattern = table.concat(AI.OpponentModel.lastActions, ",", #AI.OpponentModel.lastActions - 2, #AI.OpponentModel.lastActions)
              if not AI.LSTMOpponentModel.patterns[pattern] then
                  AI.LSTMOpponentModel.patterns[pattern] = {}
                  AI.LSTMOpponentModel.patternConfidence[pattern] = 0
              end
              -- Store what action followed this pattern
              table.insert(AI.LSTMOpponentModel.patterns[pattern], observedAction)
              AI.LSTMOpponentModel.patternConfidence[pattern] += 1
          end
      end
  end
  
  -- Hybrid learning: Update transition model
  function AI.Learning.UpdateTransitionModel(state, action, nextState, reward)
      if not AI.HybridModel.useModelBased then return end
      
      -- Create state signature (simplified)
      local stateKey = string.format("H%.1f_D%.0f_E%.0f", 
          state.myHealthPercent or 0, 
          state.distance or 0, 
          state.energy or 0)
      local nextStateKey = string.format("H%.1f_D%.0f_E%.0f", 
          nextState.myHealthPercent or 0, 
          nextState.distance or 0, 
          nextState.energy or 0)
      
      -- Update transition counts
      local transitionKey = stateKey .. "_" .. action
      if not AI.HybridModel.transitionCounts[transitionKey] then
          AI.HybridModel.transitionCounts[transitionKey] = {}
      end
      AI.HybridModel.transitionCounts[transitionKey][nextStateKey] = 
          (AI.HybridModel.transitionCounts[transitionKey][nextStateKey] or 0) + 1
      
      -- Update reward model
      if not AI.HybridModel.rewardModel[transitionKey] then
          AI.HybridModel.rewardModel[transitionKey] = { sum = 0, count = 0 }
      end
      AI.HybridModel.rewardModel[transitionKey].sum += reward
      AI.HybridModel.rewardModel[transitionKey].count += 1
      
      -- Update model confidence based on data collected
      local totalTransitions = 0
      for _, nextStates in pairs(AI.HybridModel.transitionCounts) do
          for _, count in pairs(nextStates) do
              totalTransitions += count
          end
      end
      AI.HybridModel.modelConfidence = math.min(0.8, totalTransitions / 100)
  end
  
  -- Model-based planning: Simulate future outcomes
  function AI.Learning.SimulateFuture(state, action, tactic, depth)
      if depth <= 0 or not AI.HybridModel.useModelBased then return 0 end
      
      local stateKey = string.format("H%.1f_D%.0f_E%.0f", 
          state.myHealthPercent or 0, 
          state.distance or 0, 
          state.energy or 0)
      local transitionKey = stateKey .. "_" .. action
      
      -- Get expected reward from model
      local expectedReward = 0
      if AI.HybridModel.rewardModel[transitionKey] then
          local model = AI.HybridModel.rewardModel[transitionKey]
          expectedReward = model.sum / model.count
      end
      
      -- Simple recursive planning (limited depth)
      if depth > 1 and AI.HybridModel.transitionCounts[transitionKey] then
          local futureValue = 0
          local totalCount = 0
          
          -- Average over possible next states
          for nextStateKey, count in pairs(AI.HybridModel.transitionCounts[transitionKey]) do
              totalCount += count
          end
          
          if totalCount > 0 then
              futureValue = AI.Learning.GetBestNextQValue(tactic, {
                  ATTACK = true, SPECIAL = true, EVADE = true, FEINT = true, REPOSITION = true
              })
          end
          
          return expectedReward + CONST.DISCOUNT_FACTOR_GAMMA * futureValue
      end
      
      return expectedReward
  end

  --// --- EVALUATION MODULE ---
  function AI.Evaluation.ChooseTactic(state)
      local now = os.clock()

      -- Highest priority: Glitch exploitation.
      if AI.GlitchScanner.isGlitchDetected and now - AI.GlitchScanner.detectionTime < 5.0 then
          if AI.State.currentTactic ~= "ExploitSeeker" then
              print(`GLITCH DETECTED (${AI.GlitchScanner.glitchType})! Forcing EXPLOITSEEKER tactic.`)
              -- Reinforce the action that *caused* the glitch.
              AI.Learning.Reinforce("ExploitSeeker", AI.State.lastAction, CONST.GLITCH_DETECTION_REWARD, 0) -- No future Q-value needed here.
          end
          return "ExploitSeeker"
      end

      -- Cooldown to prevent rapid tactic switching.
      if now - AI.State.lastTacticChangeTime < CONST.TACTIC_CHANGE_COOLDOWN then
          return AI.State.currentTactic
      end

      -- If confidence is critical, switch to experimental mode.
      if AI.State.tacticConfidence < CONST.EXPLOIT_SEEKER_CONFIDENCE_THRESHOLD and now - AI.State.lastExploitSeekerTriggerTime > CONST.EXPLOIT_SEEKER_COOLDOWN then
          AI.State.lastTacticChangeTime = now
          AI.State.lastExploitSeekerTriggerTime = now
          print("Confidence critical. Switching to EXPLOITSEEKER to find new strategies.")
          return "ExploitSeeker"
      end

      -- If confidence is high, stick with the current tactic for a bit longer.
      if AI.State.tacticConfidence > CONST.TACTIC_CONFIDENCE_THRESHOLD and os.clock() - AI.State.lastTacticChangeTime < CONST.TACTIC_CHANGE_COOLDOWN * 2 then
          return AI.State.currentTactic
      end

      -- Rule-based tactic selection.
      local newTactic
      if state.targetHealthPercent < 0.2 and state.distance < CONST.ATTACK_RANGE * 1.5 then newTactic = "Finisher"
      elseif state.myHealthPercent > 0.75 and state.energy > (CONST.MAX_ENERGY * 0.6) then newTactic = "Aggressive"
      elseif state.myHealthPercent < 0.35 or state.energy < (CONST.MAX_ENERGY * 0.25) then newTactic = "Defensive"
      else newTactic = "BaitAndPunish" end

      if newTactic ~= AI.State.currentTactic then
          AI.State.lastTacticChangeTime = now
          AI.State.tacticConfidence = 100 -- Reset confidence on switch.
          print("Tactic changed to:", newTactic)
      end
      return newTactic
  end

  function AI.Evaluation.FindBestActionByScore(state, tactic)
      local now = os.clock()

      -- Immediate capitalization windows (glitch or punish) - highest priority
      local function isCapitalizeWindow()
          return now < AI.State.glitchComboWindowEndTime or now < AI.State.punishWindowEndTime
      end

      if isCapitalizeWindow() and state.inRange then
          if AI.State.glitchComboWindowEndTime > now then print("Executing Glitch Combo!") end
          if #AI.AvailableMoves > 0 and now - AI.State.lastActionTimes.SPECIAL > CONST.SPECIAL_MOVE_COOLDOWN and state.energy >= CONST.COSTS.SPECIAL then return "SPECIAL" end
          if now - AI.State.lastActionTimes.ATTACK > CONST.ATTACK_COOLDOWN then return "ATTACK" end
      end

      -- Cache possible actions check for ultra-fast performance
      local possibleActions = {}
      local inCombo = now < AI.State.comboStateEndTime
      possibleActions.ATTACK = (now - AI.State.lastActionTimes.ATTACK > (inCombo and CONST.ATTACK_COOLDOWN * 0.5 or CONST.ATTACK_COOLDOWN)) and state.inRange
      possibleActions.SPECIAL = (now - AI.State.lastActionTimes.SPECIAL > CONST.SPECIAL_MOVE_COOLDOWN) and (state.energy >= CONST.COSTS.SPECIAL) and state.inRange and (#AI.AvailableMoves > 0)
      possibleActions.EVADE = (now - AI.State.lastActionTimes.EVADE > CONST.DASH_COOLDOWN) and (state.energy >= CONST.COSTS.DASH)
      possibleActions.FEINT = (now - AI.State.lastActionTimes.FEINT > CONST.FEINT_COOLDOWN) and (state.energy >= CONST.COSTS.FEINT) and state.inRange
      possibleActions.REPOSITION = true

      -- ExploitSeeker uses pure random exploration
      if tactic == "ExploitSeeker" then
          local actionPool = {}
          for action, available in pairs(possibleActions) do
              if available and (action == "ATTACK" or action == "SPECIAL" or action == "EVADE" or action == "FEINT") then
                  table.insert(actionPool, action)
              end
          end
          return #actionPool > 0 and actionPool[math.random(#actionPool)] or "REPOSITION"
      end

      -- Advanced exploration strategies with deep learning
      local tacticStatsA = AI.ActionStats[tactic]
      local tacticStatsB = AI.QNetworkB[tactic]
      local actionScores = {}
      
      -- Extract deep features for better decision-making
      local features, featureScore = AI.Learning.ExtractFeatures(state)
      
      -- Get deep neural network predictions
      local stateEncoded = AI.DeepLearning.EncodeState(state)
      local dnnQValues = AI.DeepLearning.PredictQValues(stateEncoded, false)
      local ensembleQValues = AI.DeepLearning.EnsemblePredict(stateEncoded)
      local policyProbs = AI.DeepLearning.PredictPolicy(stateEncoded)
      
      -- Choose exploration strategy based on learning progress
      local explorationMode = AI.State.explorationStrategy
      if AI.ActionStats.totalTrials < 100 then
          explorationMode = "epsilon-greedy" -- Early exploration
      elseif AI.ActionStats.totalTrials < 500 then
          explorationMode = "policy-gradient" -- Use policy network
      else
          explorationMode = "ensemble" -- Late-game refinement with ensemble
      end

      -- Build action scores with deep learning integration
      local actionMap = {ATTACK=1, SPECIAL=2, EVADE=3, FEINT=4, REPOSITION=5}
      for action, available in pairs(possibleActions) do
          if available then
              local statsA = tacticStatsA[action]
              local statsB = tacticStatsB[action]
              local actionIdx = actionMap[action]
              local score
              
              if statsA.count == 0 then
                  -- Use deep network prediction for untried actions
                  score = CONST.UNTRIED_ACTION_SCORE + (dnnQValues[actionIdx] or 0) * 10
              else
                  -- Hybrid approach: Combine tabular Q-learning with deep network
                  local qValueA = statsA.value / statsA.count
                  local qValueB = (statsB and statsB.count > 0) and (statsB.value / statsB.count) or qValueA
                  local tabularQ = (qValueA + qValueB) / 2
                  
                  -- Deep network Q-value (ensemble average)
                  local deepQ = (dnnQValues[actionIdx] + ensembleQValues[actionIdx]) / 2
                  
                  -- Adaptive weighting: More weight on deep network as it learns
                  local deepWeight = math.min(0.7, AI.ActionStats.totalTrials / 1000)
                  local qValue = tabularQ * (1 - deepWeight) + deepQ * deepWeight
                  
                  -- Hybrid learning: Combine model-free and model-based
                  if AI.HybridModel.useModelBased and AI.HybridModel.modelConfidence > 0.2 then
                      local modelValue = AI.Learning.SimulateFuture(state, action, tactic, AI.State.modelBasedPlanDepth)
                      qValue = qValue * (1 - AI.HybridModel.modelConfidence) + modelValue * AI.HybridModel.modelConfidence
                  end
                  
                  -- Add policy network guidance (Actor-Critic)
                  local policyBonus = (policyProbs[actionIdx] or 0) * 15
                  
                  -- Add deep feature score
                  score = qValue + featureScore * 0.15 + policyBonus
              end

              -- Apply situational modifiers for tactical awareness
              local modifier = 1.0
              
              -- React to predicted opponent action
              if AI.OpponentModel.predictedNextAction == "ATTACK" and action == "EVADE" then
                  modifier *= 1.5
              end
              
              if action == "EVADE" and state.isTargetAttacking then 
                  modifier = 2.2 -- Stronger preference for evasion when under attack
              end
              if (tactic == "BaitAndPunish" or tactic == "Defensive") and action == "EVADE" then 
                  modifier *= 1.4
              end
              if action == "ATTACK" and inCombo then
                  modifier *= 1.5 -- Stronger combo encouragement
              end
              if action == "SPECIAL" and now < AI.State.punishWindowEndTime then
                  modifier *= 1.6 -- Capitalize on punish windows
              end

              table.insert(actionScores, { 
                  action = action, 
                  score = score * modifier,
                  count = statsA.count,
                  rawQ = (statsA.count > 0) and (statsA.value / statsA.count) or 0
              })
          end
      end

      if #actionScores == 0 then return "REPOSITION" end

      -- Apply exploration strategy
      if explorationMode == "epsilon-greedy" and math.random() < AI.State.epsilon then
          -- Epsilon-greedy: Random action
          return actionScores[math.random(#actionScores)].action
      elseif explorationMode == "policy-gradient" then
          -- Policy gradient: Sample from policy network distribution
          local rand = math.random()
          local cumProb = 0
          for i, actionData in ipairs(actionScores) do
              local actionIdx = actionMap[actionData.action]
              cumProb += policyProbs[actionIdx] or 0
              if cumProb >= rand then
                  return actionData.action
              end
          end
      elseif explorationMode == "ensemble" then
          -- Ensemble voting: Use ensemble consensus with confidence weighting
          for _, actionData in ipairs(actionScores) do
              local actionIdx = actionMap[actionData.action]
              -- Weight by ensemble agreement (lower variance = higher confidence)
              local variance = math.abs(dnnQValues[actionIdx] - ensembleQValues[actionIdx])
              local confidenceBonus = math.max(0, 5 - variance)
              actionData.score += confidenceBonus
          end
      elseif explorationMode == "ucb" then
          -- Upper Confidence Bound: Balance exploration and exploitation
          for _, actionData in ipairs(actionScores) do
              if actionData.count > 0 then
                  local exploration = CONST.UCB_EXPLORATION_CONSTANT * math.sqrt(math.log(AI.ActionStats.totalTrials) / actionData.count)
                  actionData.score += exploration
              end
          end
      elseif explorationMode == "boltzmann" then
          -- Boltzmann exploration: Probabilistic selection based on Q-values
          local expScores = {}
          local sumExp = 0
          for _, actionData in ipairs(actionScores) do
              local expScore = math.exp(actionData.score / AI.State.boltzmannTemperature)
              table.insert(expScores, expScore)
              sumExp += expScore
          end
          
          -- Probabilistic selection
          local rand = math.random() * sumExp
          local cumSum = 0
          for i, expScore in ipairs(expScores) do
              cumSum += expScore
              if cumSum >= rand then
                  return actionScores[i].action
              end
          end
      end

      -- Sort by score (highest first) and return best
      table.sort(actionScores, function(a, b) return a.score > b.score end)
      return actionScores[1].action
  end

  --// --- ACTION MODULE ---
  function AI.Actions.Execute(action, state)
      local now = os.clock()
      AI.State.lastAction = action

      if not AI.State.targetTorso then return end -- Safety check
      local targetPos = AI.State.targetTorso.Position

      local cost = CONST.COSTS[action:upper()] or 0
      AI.State.currentEnergy -= cost
      
      -- Store current state for experience replay
      AI.State.actionsSinceReplay += 1

      if action == "FEINT" then
          communicateEvent:FireServer({["Goal"]="Feint"})
          AI.State.lastActionTimes.FEINT = now
          AI.State.isFeinting, AI.State.feintStartTime, AI.State.distanceAtFeintStart = true, now, (rootPart.Position - targetPos).Magnitude
      elseif action == "ATTACK" then
          communicateEvent:FireServer({["Mobile"]=true, ["Goal"]="LeftClick"})
          AI.State.lastActionTimes.ATTACK = now
          AI.State.punishWindowEndTime = 0
      elseif action == "SPECIAL" then
          if #AI.AvailableMoves > 0 then
              local moveName = AI.AvailableMoves[math.random(#AI.AvailableMoves)]
              local tool = player.Backpack:FindFirstChild(moveName) or character:FindFirstChild(moveName)
              if tool then
                  communicateEvent:FireServer({["Tool"] = tool, ["Goal"] = "Console Move"})
                  AI.State.lastActionTimes.SPECIAL = now
                  AI.State.punishWindowEndTime = 0
              end
          end
      elseif action == "EVADE" then
          -- Smarter evade direction based on target velocity
          local targetVelocity = AI.State.targetTorso.AssemblyLinearVelocity
          local perpendicular = rootPart.CFrame.RightVector * (math.random() > 0.5 and 1 or -1)
          if targetVelocity.Magnitude > 10 then
              -- Dodge perpendicular to target's movement (+90 degree rotation in horizontal plane)
              local targetDir = targetVelocity.Unit
              perpendicular = Vector3.new(-targetDir.Z, 0, targetDir.X)
          end
          communicateEvent:FireServer({["Dash"]=perpendicular.Unit, ["Key"]="Q", ["Goal"]="KeyPress"})
          AI.State.lastActionTimes.EVADE = now
          AI.State.isPerfectDodging, AI.State.dodgeStartTime = true, now
      elseif action == "REPOSITION" then
          if now - AI.State.lastStrafeChange > (0.8 + math.random() * 0.4) then -- Faster strafe changes
              AI.State.strafeDirection *= -1
              AI.State.lastStrafeChange = now
          end

          local currentDist = (rootPart.Position - targetPos).Magnitude
          local desiredRange
          if AI.State.currentTactic == "Defensive" then desiredRange = CONST.KITE_RANGE
          elseif AI.State.currentTactic == "Finisher" or AI.State.currentTactic == "Aggressive" then desiredRange = CONST.ATTACK_RANGE * 0.7
          else desiredRange = CONST.ATTACK_RANGE * 0.9 end

          local moveDirection = (targetPos - ((targetPos - rootPart.Position).Unit * desiredRange) + (rootPart.CFrame.RightVector * 10 * AI.State.strafeDirection))
          humanoid:MoveTo(moveDirection)

          -- Immediate feedback for repositioning
          local distanceError = math.abs(currentDist - desiredRange)
          local reward = math.max(0, 1 - (distanceError / desiredRange)) * 2
          AI.Learning.Reinforce(AI.State.currentTactic, "REPOSITION", reward, state)
      end
      
      -- Store current state in history for n-step returns
      table.insert(AI.State.stateHistory, {state = state, action = action, reward = 0, tactic = AI.State.currentTactic})
      if #AI.State.stateHistory > CONST.N_STEP_RETURN + 2 then
          table.remove(AI.State.stateHistory, 1)
      end
      
      -- Store experience for replay learning and hybrid model
      if state then
          AI.Learning.StoreExperience(state, action, 0, state, AI.State.currentTactic)
          
          -- Update hybrid transition model
          if AI.ActionStats.totalTrials % CONST.MODEL_UPDATE_THRESHOLD == 0 then
              AI.Learning.UpdateTransitionModel(state, action, state, 0)
          end
      end
      
      -- Perform experience replay periodically
      if AI.State.actionsSinceReplay >= CONST.REPLAY_FREQUENCY then
          AI.Learning.ReplayExperiences()
          AI.State.actionsSinceReplay = 0
      end
  end

  --// ================= HELPER & UPDATE FUNCTIONS ================= //

  local function Cleanup()
      if targetDiedConnection then targetDiedConnection:Disconnect(); targetDiedConnection = nil end
      for _, c in ipairs(connections) do c:Disconnect() end
      connections = {}
      AI.State.combatState = "IDLE"
      AI.State.currentTarget = nil
      print("Project Apex has been shut down for this character.")
  end

  local function GetTargetScore(targetCharacter)
      -- Fast validity checks with early returns
      if not targetCharacter or not targetCharacter.Parent then return 0 end
      local root = targetCharacter:FindFirstChild("HumanoidRootPart")
      if not root then return 0 end
      
      local targetHumanoid = targetCharacter:FindFirstChildOfClass("Humanoid")
      if not targetHumanoid or targetHumanoid.Health <= 0 then return 0 end

      -- Pre-calculate distance once
      local distance = (rootPart.Position - root.Position).Magnitude
      if distance > CONST.AGGRO_RANGE then return 0 end

      -- Optimized scoring with weighted factors
      local healthRatio = targetHumanoid.Health / targetHumanoid.MaxHealth
      local healthScore = (1 - healthRatio) * 2.0 -- Increased weight for low-health targets
      local distanceScore = (1 - (distance / CONST.AGGRO_RANGE)) * 1.5 -- Closer is better
      
      -- Improved velocity penalty with gradual scaling
      local velocityMag = root.AssemblyLinearVelocity.Magnitude
      local velocityScore = velocityMag > 25 and -(velocityMag / 50) or 0.2 -- Bonus for stationary targets

      return healthScore + distanceScore + velocityScore
  end

  local function EvaluateAndSetTarget()
      local bestTarget, highestScore = nil, -math.huge
      for _, p in ipairs(Players:GetPlayers()) do
          if p ~= player and p.Character then
              local score = GetTargetScore(p.Character)
              if score > highestScore then
                  highestScore, bestTarget = score, p.Character
              end
          end
      end

      if bestTarget and bestTarget ~= AI.State.currentTarget then
          if targetDiedConnection then targetDiedConnection:Disconnect() end

          local targetHumanoid = bestTarget:FindFirstChildOfClass("Humanoid")
          AI.State.currentTarget, AI.State.targetHumanoid, AI.State.targetTorso = bestTarget, targetHumanoid, bestTarget:FindFirstChild("Torso") or bestTarget:FindFirstChild("HumanoidRootPart")
          AI.State.lastTargetHealth = targetHumanoid and targetHumanoid.Health or 0

          targetDiedConnection = targetHumanoid.Died:Connect(function()
              AI.Score += 10
              print(`Target eliminated. New Score: {AI.Score}`)
              AI.State.currentTarget = nil
          end)
      end

      if not AI.State.currentTarget or not AI.State.targetHumanoid or AI.State.targetHumanoid.Health <= 0 then
          AI.State.combatState = "IDLE"
          AI.State.currentTarget = nil
      else
          AI.State.combatState = "ENGAGING"
      end
  end

  -- Helper function to detect NaN values (IEEE 754: NaN is the only value that doesn't equal itself)
  local function isNaN(value)
      return value ~= value
  end

  local function RunGlitchScanner()
      local now = os.clock()
      
      -- Quick exit if glitch is already active
      if AI.GlitchScanner.isGlitchDetected then
          if now - AI.GlitchScanner.detectionTime > 2.0 then
              AI.GlitchScanner.isGlitchDetected = false
          end
          return
      end

      local function triggerGlitch(glitchType)
          AI.GlitchScanner.isGlitchDetected = true
          AI.GlitchScanner.glitchType = glitchType
          AI.GlitchScanner.detectionTime = now
          AI.State.glitchComboWindowEndTime = now + CONST.GLITCH_COMBO_WINDOW_DURATION
          print(`GLITCH DETECTED: {glitchType}`)
      end

      -- 1. Fast velocity spike detection (optimized)
      local currentVelocity = rootPart.AssemblyLinearVelocity
      local currentVelocityMag = currentVelocity.Magnitude
      if currentVelocityMag > CONST.GLITCH_VELOCITY_THRESHOLD and AI.GlitchScanner.lastVelocityMagnitude < CONST.GLITCH_VELOCITY_THRESHOLD then
          triggerGlitch("Velocity Spike")
          AI.GlitchScanner.lastVelocityMagnitude = currentVelocityMag
          return
      end
      AI.GlitchScanner.lastVelocityMagnitude = currentVelocityMag

      -- 2. Humanoid state anomaly detection (optimized with early return)
      local currentState = humanoid:GetState()
      if currentState ~= AI.GlitchScanner.lastHumanoidState then
          if currentState == Enum.HumanoidStateType.StrafingNoPhysics or currentState == Enum.HumanoidStateType.None then
              triggerGlitch("Humanoid State Anomaly")
              AI.GlitchScanner.lastHumanoidState = currentState
              return
          end
          AI.GlitchScanner.lastHumanoidState = currentState
      end

      -- 3. NaN position check (fast)
      local pos = rootPart.Position
      if isNaN(pos.X) or isNaN(pos.Y) or isNaN(pos.Z) then
          triggerGlitch("NaN Position")
      end
  end

  local function UpdateAIState(deltaTime)
      local now = os.clock()
      AI.State.currentEnergy = math.min(CONST.MAX_ENERGY, AI.State.currentEnergy + CONST.ENERGY_REGEN_RATE * deltaTime)

      if now - AI.State.lastTargetScanTime > CONST.TARGET_SCAN_INTERVAL then
          EvaluateAndSetTarget()
          AI.State.lastTargetScanTime = now
      end

      if AI.State.currentTarget and AI.State.currentTarget.Parent then
          local activeTool = AI.State.currentTarget:FindFirstChildOfClass("Tool")
          local wasAttacking = AI.State.isTargetAttacking
          AI.State.isTargetAttacking = activeTool and activeTool.Enabled
          
          -- Detect opponent actions for modeling
          if AI.State.isTargetAttacking and not wasAttacking then
              AI.Learning.UpdateOpponentModel("ATTACK")
              AI.State.lastOpponentAction = "ATTACK"
              AI.State.opponentActionTime = now
          elseif AI.State.targetTorso then
              local targetVelocity = AI.State.targetTorso.AssemblyLinearVelocity.Magnitude
              if targetVelocity > 50 and now - AI.State.opponentActionTime > 0.5 then
                  AI.Learning.UpdateOpponentModel("EVADE")
                  AI.State.lastOpponentAction = "EVADE"
                  AI.State.opponentActionTime = now
              end
          end
      else
          AI.State.isTargetAttacking = false
      end

      RunGlitchScanner() -- Run the scanner every state update.
  end

  local function UpdateLearningAndRewards(now)
      -- Fast exit if no valid target
      if not AI.State.currentTarget or not AI.State.targetHumanoid or not AI.State.targetTorso then return end

      local tactic = AI.State.currentTactic

      -- Feint evaluation (optimized timing check)
      if AI.State.isFeinting and now > AI.State.feintStartTime + CONST.FEINT_DURATION then
          AI.State.isFeinting = false
          local currentDistance = (rootPart.Position - AI.State.targetTorso.Position).Magnitude
          local feintSucceeded = currentDistance > AI.State.distanceAtFeintStart + CONST.FEINT_SUCCESS_DISTANCE
          local reward = feintSucceeded and CONST.REWARD_FEINT_SUCCESS or CONST.PENALTY_FAILED_ACTION
          
          -- Build current state for reinforcement
          local currentState = {
              myHealthPercent = humanoid.Health / humanoid.MaxHealth,
              targetHealthPercent = AI.State.targetHumanoid.Health / AI.State.targetHumanoid.MaxHealth,
              distance = currentDistance,
              energy = AI.State.currentEnergy,
              inRange = currentDistance <= CONST.ATTACK_RANGE,
              isTargetAttacking = AI.State.isTargetAttacking,
          }
          AI.Learning.Reinforce(tactic, "FEINT", reward, currentState)
          if feintSucceeded then
              AI.State.punishWindowEndTime = now + CONST.PUNISH_WINDOW_DURATION
              print("Feint successful! Punish window opened.")
          end
      end

      -- Perfect dodge window timeout
      if AI.State.isPerfectDodging and now > AI.State.dodgeStartTime + CONST.PERFECT_DODGE_WINDOW then
          AI.State.isPerfectDodging = false
      end

      -- Damage dealt evaluation with hybrid learning and feature weight adaptation
      local currentTargetHealth = AI.State.targetHumanoid.Health
      local damageDealt = AI.State.lastTargetHealth - currentTargetHealth
      if damageDealt > 0 then
          local action = AI.State.lastAction
          local reward
          
          -- Anomaly detection for unexpected damage sources
          if action ~= "ATTACK" and action ~= "SPECIAL" then
              print(`ANOMALY: Damage (${damageDealt}) dealt via ${action}. High reward!`)
              reward = CONST.REWARD_ANOMALOUS_DAMAGE
          else
              reward = damageDealt * CONST.REWARD_DAMAGE_DEALT_MULTIPLIER
              -- Bonus for combo damage
              if now < AI.State.comboStateEndTime then
                  reward *= 1.35
              end
              -- Bonus for exploiting opportunity windows
              if now < AI.State.punishWindowEndTime then
                  reward *= 1.25
              end
          end

          -- Build next state for deep learning
          local nextState = {
              myHealthPercent = humanoid.Health / humanoid.MaxHealth,
              targetHealthPercent = AI.State.targetHumanoid.Health / AI.State.targetHumanoid.MaxHealth,
              distance = (rootPart.Position - AI.State.targetTorso.Position).Magnitude,
              energy = AI.State.currentEnergy,
              inRange = (rootPart.Position - AI.State.targetTorso.Position).Magnitude <= CONST.ATTACK_RANGE,
              isTargetAttacking = AI.State.isTargetAttacking,
          }
          
          -- Reinforce with next state for deep learning
          AI.Learning.Reinforce(tactic, action, reward, nextState)

          -- Extend combo window
          AI.State.comboStateEndTime = now + CONST.COMBO_WINDOW_DURATION
          
          -- Adaptive feature weight tuning based on success
          AI.FeatureWeights.tacticSuccess = math.min(2.5, AI.FeatureWeights.tacticSuccess * 1.08)
          AI.FeatureWeights.comboDensity = math.min(2.0, AI.FeatureWeights.comboDensity * 1.05)
          AI.FeatureWeights.opportunityWindow = math.min(2.2, AI.FeatureWeights.opportunityWindow * 1.06)
          
          -- Adjust feature weights based on context
          if now < AI.State.punishWindowEndTime then
              AI.FeatureWeights.opportunityWindow = math.min(2.5, AI.FeatureWeights.opportunityWindow * 1.1)
          end
      else
          -- Decay feature weights if no damage (adaptive forgetting)
          AI.FeatureWeights.tacticSuccess = math.max(0.4, AI.FeatureWeights.tacticSuccess * 0.995)
          AI.FeatureWeights.comboDensity = math.max(0.5, AI.FeatureWeights.comboDensity * 0.997)
      end
      
      AI.State.lastTargetHealth = currentTargetHealth
  end

  local function ExecuteCombatLogic(now)
      if AI.State.combatState ~= "ENGAGING" or AI.State.isFeinting then return end
      if not (AI.State.targetTorso and AI.State.targetTorso.Parent and AI.State.targetHumanoid) then return end

      -- Ultra-optimized prediction with aggressive caching
      local shouldRecalcPrediction = not AI.State.cachedPredictedPos or (now - AI.State.lastPredictionTime) > 0.08
      if shouldRecalcPrediction then
          local ping = player:GetNetworkPing()
          local velocity = AI.State.targetTorso.AssemblyLinearVelocity
          -- Simplified prediction for speed
          AI.State.cachedPredictedPos = AI.State.targetTorso.Position + velocity * (CONST.BASE_PREDICTION_TIME + ping * 0.5)
          AI.State.lastPredictionTime = now
      end
      
      -- Ultra-fast aim update (single operation)
      local targetPos = AI.State.cachedPredictedPos
      rootPart.CFrame = CFrame.lookAt(rootPart.Position, Vector3.new(targetPos.X, rootPart.Position.Y, targetPos.Z))

      -- Pre-calculate values once for entire decision cycle
      local distance = (rootPart.Position - AI.State.targetTorso.Position).Magnitude
      local myHealthPercent = humanoid.Health / humanoid.MaxHealth
      local targetHealthPercent = AI.State.targetHumanoid.Health / AI.State.targetHumanoid.MaxHealth
      
      -- Build current state object for decision making (ultra-optimized)
      local state = {
          myHealthPercent = myHealthPercent,
          targetHealthPercent = targetHealthPercent,
          distance = distance,
          energy = AI.State.currentEnergy,
          inRange = distance <= CONST.ATTACK_RANGE,
          isTargetAttacking = AI.State.isTargetAttacking,
      }

      -- Single-pass decision pipeline
      AI.State.currentTactic = AI.Evaluation.ChooseTactic(state)
      local bestAction = AI.Evaluation.FindBestActionByScore(state, AI.State.currentTactic)
      AI.Actions.Execute(bestAction, state)
  end

  --// ================= EVENT CONNECTIONS ================= //
  table.insert(connections, humanoid.HealthChanged:Connect(function(newHealth)
      local damageTaken = AI.State.myHealth - newHealth
      
      -- Build current state
      local currentState = nil
      if AI.State.targetHumanoid and AI.State.targetTorso then
          currentState = {
              myHealthPercent = newHealth / humanoid.MaxHealth,
              targetHealthPercent = AI.State.targetHumanoid.Health / AI.State.targetHumanoid.MaxHealth,
              distance = (rootPart.Position - AI.State.targetTorso.Position).Magnitude,
              energy = AI.State.currentEnergy,
              inRange = (rootPart.Position - AI.State.targetTorso.Position).Magnitude <= CONST.ATTACK_RANGE,
              isTargetAttacking = AI.State.isTargetAttacking,
          }
      end
      
      if damageTaken > 0 then
          local penalty = damageTaken * CONST.PENALTY_DAMAGE_TAKEN_MULTIPLIER
          local failedAction = AI.State.lastAction
          if AI.State.isPerfectDodging and os.clock() > AI.State.dodgeStartTime then -- Check if dodge was active
              failedAction = "EVADE"
              AI.Learning.Reinforce(AI.State.currentTactic, "EVADE", CONST.PENALTY_FAILED_ACTION, currentState)
          else
              AI.Learning.Reinforce(AI.State.currentTactic, failedAction, penalty, currentState)
          end
          AI.State.isPerfectDodging = false
      elseif AI.State.isPerfectDodging and AI.State.isTargetAttacking then
          AI.Learning.Reinforce(AI.State.currentTactic, "EVADE", CONST.REWARD_PERFECT_DODGE, currentState)
          AI.State.punishWindowEndTime = os.clock() + CONST.PUNISH_WINDOW_DURATION
          print("Perfect Dodge! Opening Punish Window.")
          AI.State.isPerfectDodging = false
      end
      AI.State.myHealth = newHealth
  end))

  table.insert(connections, LogService.MessageOut:Connect(function(message, messageType)
      if messageType == Enum.MessageType.MessageError and not AI.GlitchScanner.isGlitchDetected then
          local now = os.clock()
          AI.GlitchScanner.isGlitchDetected = true
          AI.GlitchScanner.glitchType = "Game Script Error"
          AI.GlitchScanner.detectionTime = now
          AI.State.glitchComboWindowEndTime = now + CONST.GLITCH_COMBO_WINDOW_DURATION
      end
  end))

  table.insert(connections, humanoid.Died:Connect(function()
      AI.Score -= 10
      print(`Project Apex defeated. New Score: {AI.Score}.`)
      Cleanup()
  end))
  table.insert(connections, character.AncestryChanged:Connect(function(_, parent)
      if not parent then Cleanup() end
  end))

  -- The main heartbeat loop that drives the AI.
  table.insert(connections, RunService.Heartbeat:Connect(function(deltaTime)
      local success, err = pcall(function()
          local now = os.clock()
          UpdateAIState(deltaTime)
          UpdateLearningAndRewards(now)
          ExecuteCombatLogic(now)
      end)
      if not success then
          warn("Project Apex encountered an error in its main loop:", err)
      end
  end))
end

--// ================= SCRIPT ENTRY POINT ================= //
-- Connect the initialization function to the CharacterAdded event.
player.CharacterAdded:Connect(initializeCombatSystem)
-- If the character already exists when the script runs, initialize immediately.
if player.Character then
  initializeCombatSystem(player.Character)
end
