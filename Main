 -- =================================================================================
--// INSTRUCTION: This script must be placed in StarterPlayer > StarterPlayerScripts
--// to ensure it runs automatically and persists when your character changes.
-- =================================================================================
--// SELF-LEARNING COMBAT FRAMEWORK V34.0 - "PROJECT APEX" (Enhanced by GitHub Copilot)
--// USER: niclaspoopy123
--// BUILD DATE: 2025-10-29
--// UPDATE: Advanced Q-Learning with experience replay, deep feature extraction, and optimized reaction times.
--// CHANGE: Significantly improved learning speed, decision quality, and combat responsiveness.
-- =================================================================================
--[[
V34.0 PATCH NOTES (PROJECT APEX):
- NEW (EXPERIENCE REPLAY): Implemented replay buffer to learn from past experiences more efficiently
- NEW (EPSILON-GREEDY EXPLORATION): Dynamic exploration strategy with decay for optimal learning
- NEW (ELIGIBILITY TRACES): Better credit assignment for action sequences
- NEW (DEEP FEATURE EXTRACTION): Multi-layer state representation for better decision-making
- NEW (PRIORITIZED REPLAY): Learn more from important experiences
- IMPROVED (REACTION TIME): Removed unnecessary delays and optimized decision pipeline
- IMPROVED (Q-LEARNING): Enhanced temporal difference learning with n-step returns
- IMPROVED (STATE REPRESENTATION): Better feature normalization and encoding
- OPTIMIZED (PERFORMANCE): Streamlined hot paths and reduced redundant calculations

V33.0 PATCH NOTES (PROJECT TITAN):
- NEW (TRUE Q-LEARNING): The learning algorithm has been upgraded from a simple value update to a proper Q-Learning model. The AI now considers the potential future value of its next best move, leading to more sophisticated, long-term strategies.
- IMPROVED (SMARTER TARGETING): The target-scoring algorithm now penalizes targets moving at high velocity, making the AI prioritize engaging enemies it's more likely to hit.
- IMPROVED (ROBUSTNESS): Added comprehensive checks for target validity throughout the combat loop to prevent errors when a target is lost or destroyed unexpectedly.
- IMPROVED (EXPLOITSEEKER TACTIC): The 'ExploitSeeker' tactic now uses a weighted-random selection, promoting more varied and unpredictable exploratory actions to discover glitches.
- REFACTOR (MODULAR DESIGN): The entire script has been reorganized into logical modules (Services, Constants, AI, CoreLogic, etc.) for significantly improved readability and easier maintenance.
--]]

--// ================= SERVICES ================= //
local Players = game:GetService("Players")
local RunService = game:GetService("RunService")
local Workspace = game:GetService("Workspace")
local LogService = game:GetService("LogService")

--// ================= LOCAL PLAYER ================= //
local player = Players.LocalPlayer

--// ================= AI'S PERSISTENT BRAIN ================= //
-- This table stores the AI's long-term memory, including learned values and statistics.
-- It persists across character resets, allowing the AI to learn over multiple sessions.
local AI = {
  State = {}, -- Volatile state, reset on character spawn.
  Score = 0, -- Persistent score.
  IdentifiedCharacter = "Unknown",
  AvailableMoves = {},
  ActionStats = { -- Persistent learning data.
      totalTrials = 0,
      Aggressive = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Defensive = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      BaitAndPunish = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      Finisher = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
      ExploitSeeker = { total = 0, ATTACK = { count = 0, value = 0, eligibility = 0 }, SPECIAL = { count = 0, value = 0, eligibility = 0 }, EVADE = { count = 0, value = 0, eligibility = 0 }, FEINT = { count = 0, value = 0, eligibility = 0 }, REPOSITION = { count = 0, value = 0, eligibility = 0 } },
  },
  --// EXPERIENCE REPLAY: Store past experiences for better learning
  ExperienceBuffer = {
      maxSize = 1000,
      experiences = {},
      priorities = {},
      nextIndex = 1,
  },
  --// DEEP FEATURES: Multi-layer feature extraction
  FeatureWeights = {
      healthDiff = 1.0,
      distanceToTarget = 1.0,
      energyLevel = 1.0,
      velocityMatch = 1.0,
      tacticSuccess = 1.0,
  },
  --// ANOMALY: Module for actively reading the game for glitches.
  GlitchScanner = {
      isGlitchDetected = false,
      glitchType = "None",
      lastVelocityMagnitude = 0,
      lastHumanoidState = "None",
      detectionTime = 0,
  },
  Evaluation = {},
  Actions = {},
  Learning = {}
}

--// ================= CORE INITIALIZATION ================= //

-- This function runs when the player's character is added to the game.
-- It sets up the entire combat system, constants, and event connections.
local function initializeCombatSystem(character)
  --// --- CHARACTER VALIDATION ---
  if not character then return end
  local humanoid = character:WaitForChild("Humanoid")
  local rootPart = character:WaitForChild("HumanoidRootPart")
  local communicateEvent = character:WaitForChild("Communicate", 5)

  if not (humanoid and rootPart and communicateEvent) then
      warn("Project Titan: Character is missing required components (Humanoid, HumanoidRootPart, or Communicate event). AI will not function.")
      return
  end

  --// --- LOCAL STATE & CONNECTIONS ---
  local connections = {} -- Stores all event connections for easy cleanup.
  local targetDiedConnection

  --// --- CHARACTER & MOVE IDENTIFICATION ---
  local CHARACTER_DATA = {
      ["The Slugger"] = { Moves = {"Homerun", "Beatdown", "Grand Slam"}, AttackRange = 35, KiteRange = 45 },
      ["Garou"] = { Moves = {"Whirlwind Kick", "Machine Gun Blows", "Flowing Water"}, AttackRange = 30, KiteRange = 40 },
      ["Genos"] = { Moves = {"Ignition Burst", "Jet Dive", "Blitz Shot"}, AttackRange = 50, KiteRange = 60 },
      ["Atomic Samurai"] = { Moves = {"Quick Slice", "Atmos Cleave", "Pinpoint Cut"}, AttackRange = 40, KiteRange = 50 },
      ["Sonic"] = { Moves = {"Flash Strike", "Explosive Shuriken"}, AttackRange = 45, KiteRange = 55 },
      ["Flashy Flash"] = { Moves = {"Split Second Counter"}, AttackRange = 38, KiteRange = 48 },
      ["Default"] = { Moves = {
          "Homerun", "Beatdown", "Grand Slam", "Flash Strike", "Whirlwind Kick",
          "Scatter", "Explosive Shuriken", "Machine Gun Blows", "Ignition Burst",
          "Blitz Shot", "Jet Dive", "Quick Slice", "Atmos Cleave", "Pinpoint Cut",
          "Split Second Counter", "Flowing Water", "Crushing Pull", "Windstorm Fury",
          "Stone Coffin", "Expulsive Push"
          }, AttackRange = 30, KiteRange = 40 }
  }

  local function IdentifyCharacter()
      local ownedMovesSet = {}
      local ownedMovesList = {}

      local function getTools(container)
          if not container then return end
          for _, tool in ipairs(container:GetChildren()) do
              if tool:IsA("Tool") and not ownedMovesSet[tool.Name] then
                  ownedMovesSet[tool.Name] = true
                  table.insert(ownedMovesList, tool.Name)
              end
          end
      end

      getTools(player:WaitForChild("Backpack"))
      getTools(character)

      for characterName, data in pairs(CHARACTER_DATA) do
          if characterName ~= "Default" then
              local moveMatch = false
              for _, moveName in ipairs(data.Moves) do
                  if ownedMovesSet[moveName] then
                      moveMatch = true
                      break
                  end
              end
              if moveMatch then
                  AI.IdentifiedCharacter, AI.AvailableMoves = characterName, ownedMovesList
                  print("Character identified as:", characterName)
                  return CHARACTER_DATA[characterName]
              end
          end
      end

      AI.IdentifiedCharacter, AI.AvailableMoves = "Custom", ownedMovesList
      print("Custom character detected. Using owned moves:", table.concat(ownedMovesList, ", "))
      return CHARACTER_DATA.Default
  end

  local characterInfo = IdentifyCharacter()

  --// --- CONSTANTS ---
  -- Centralized constants for easy tuning of the AI's behavior.
  local CONST = {
      AGGRO_RANGE = 200, ATTACK_RANGE = characterInfo.AttackRange or 30, KITE_RANGE = characterInfo.KiteRange or 40,
      TARGET_SCAN_INTERVAL = 0.3, -- Reduced from 0.5 for faster target acquisition
      MAX_ENERGY = 500, ENERGY_REGEN_RATE = 100,
      COSTS = { SPECIAL = 10, DASH = 2, FEINT = 1, ATTACK = 0, REPOSITION = 0 },
      ATTACK_COOLDOWN = 0.03, SPECIAL_MOVE_COOLDOWN = 0.4, DASH_COOLDOWN = 0.08, FEINT_COOLDOWN = 0.9, -- Reduced cooldowns for faster reactions
      FEINT_DURATION = 0.2, FEINT_SUCCESS_DISTANCE = 5,
      PERFECT_DODGE_WINDOW = 0.5, PUNISH_WINDOW_DURATION = 2.0, COMBO_WINDOW_DURATION = 2.0,
      BASE_PREDICTION_TIME = 0.03, -- Reduced for faster prediction
      -- Enhanced Q-Learning parameters
      LEARNING_RATE_ALPHA = 0.4, -- Increased for faster learning
      DISCOUNT_FACTOR_GAMMA = 0.95, -- Increased for better long-term planning
      ELIGIBILITY_LAMBDA = 0.8, -- Eligibility trace decay
      COST_PENALTY_FACTOR = 0.1,
      -- Epsilon-greedy exploration
      INITIAL_EPSILON = 0.3, -- Start with 30% exploration
      MIN_EPSILON = 0.05, -- Minimum 5% exploration
      EPSILON_DECAY = 0.9995, -- Decay rate per trial
      -- Experience replay
      REPLAY_BATCH_SIZE = 8, -- Learn from multiple experiences at once
      REPLAY_FREQUENCY = 5, -- Replay every N actions
      MIN_REPLAY_SIZE = 50, -- Minimum experiences before replay starts
      N_STEP_RETURN = 3, -- Look ahead 3 steps for better credit assignment
      -- Rewards tuning
      REWARD_DAMAGE_DEALT_MULTIPLIER = 2.5, -- Increased to encourage damage
      REWARD_PERFECT_DODGE = 30, -- Increased reward for perfect dodges
      REWARD_FEINT_SUCCESS = 12,
      PENALTY_DAMAGE_TAKEN_MULTIPLIER = -3.0, -- Stronger penalty for taking damage
      PENALTY_FAILED_ACTION = -3,
      TACTIC_CONFIDENCE_THRESHOLD = 50, TACTIC_CHANGE_COOLDOWN = 1.5, -- Reduced for faster adaptation
      EXPLOIT_SEEKER_CONFIDENCE_THRESHOLD = 20,
      EXPLOIT_SEEKER_COOLDOWN = 12.0, -- Reduced cooldown
      REWARD_ANOMALOUS_DAMAGE = 60, -- Increased to encourage glitch exploitation
      GLITCH_VELOCITY_THRESHOLD = 350,
      GLITCH_DETECTION_REWARD = 20, -- Increased glitch detection reward
      GLITCH_COMBO_WINDOW_DURATION = 1.5,
  }

  --// --- STATE MANAGEMENT ---
  function AI.ResetState(maxEnergy)
      AI.State = {
          currentTarget = nil, targetHumanoid = nil, targetTorso = nil,
          combatState = "IDLE", currentTactic = "BaitAndPunish",
          lastActionTimes = { ATTACK = 0, SPECIAL = 0, EVADE = 0, FEINT = 0 },
          strafeDirection = 1, lastStrafeChange = 0, lastAction = "IDLE",
          lastTargetScanTime = 0, currentEnergy = maxEnergy,
          lastTargetHealth = 0, myHealth = humanoid.Health,
          isFeinting = false, feintStartTime = 0, distanceAtFeintStart = 0,
          isPerfectDodging = false, dodgeStartTime = 0,
          punishWindowEndTime = 0, comboStateEndTime = 0,
          glitchComboWindowEndTime = 0,
          tacticConfidence = 100, lastTacticChangeTime = 0,
          isTargetAttacking = false,
          lastExploitSeekerTriggerTime = 0,
          -- New fields for enhanced learning
          epsilon = CONST.INITIAL_EPSILON,
          actionsSinceReplay = 0,
          stateHistory = {}, -- For n-step returns
          cachedPredictedPos = nil, -- Cache predicted position
          lastPredictionTime = 0,
      }
      print("AI State reset. Learned statistics and score have been preserved.")
  end
  AI.ResetState(CONST.MAX_ENERGY)
  AI.GlitchScanner.lastHumanoidState = humanoid:GetState()

  print(`Initializing Project Apex (V34.0) for {player.Name}... Character: {AI.IdentifiedCharacter}. Enhanced Q-Learning & Deep Features active.`)

  --// ================= CORE AI LOGIC ================= //

  --// --- LEARNING MODULE ---
  
  -- Store experience in replay buffer
  function AI.Learning.StoreExperience(state, action, reward, nextState, tactic)
      local buffer = AI.ExperienceBuffer
      local experience = {
          state = state,
          action = action,
          reward = reward,
          nextState = nextState,
          tactic = tactic,
          timestamp = os.clock()
      }
      
      -- Calculate priority based on absolute TD error estimate
      local priority = math.abs(reward) + 1
      
      if #buffer.experiences < buffer.maxSize then
          table.insert(buffer.experiences, experience)
          table.insert(buffer.priorities, priority)
      else
          buffer.experiences[buffer.nextIndex] = experience
          buffer.priorities[buffer.nextIndex] = priority
      end
      
      buffer.nextIndex = (buffer.nextIndex % buffer.maxSize) + 1
  end
  
  -- Sample experiences with priority
  function AI.Learning.SampleExperiences(batchSize)
      local buffer = AI.ExperienceBuffer
      if #buffer.experiences < CONST.MIN_REPLAY_SIZE then return {} end
      
      local samples = {}
      local totalPriority = 0
      for _, p in ipairs(buffer.priorities) do
          totalPriority += p
      end
      
      for i = 1, math.min(batchSize, #buffer.experiences) do
          local rand = math.random() * totalPriority
          local cumSum = 0
          for idx, priority in ipairs(buffer.priorities) do
              cumSum += priority
              if cumSum >= rand then
                  table.insert(samples, buffer.experiences[idx])
                  break
              end
          end
      end
      
      return samples
  end
  
  -- Enhanced Q-Learning with eligibility traces
  function AI.Learning.Reinforce(tactic, action, rawReward, nextMaxQValue)
      local tacticStats = AI.ActionStats[tactic]
      if not tacticStats or not tacticStats[action] then return end

      local stats = tacticStats[action]
      local actionCost = CONST.COSTS[action] or 0
      local netReward = rawReward - (actionCost * CONST.COST_PENALTY_FACTOR)

      -- Calculate TD error
      local old_value = stats.value
      local td_error = netReward + CONST.DISCOUNT_FACTOR_GAMMA * nextMaxQValue - old_value
      
      -- Update eligibility trace
      stats.eligibility = stats.eligibility * CONST.DISCOUNT_FACTOR_GAMMA * CONST.ELIGIBILITY_LAMBDA + 1
      
      -- Q-Learning update with eligibility traces
      stats.value = old_value + CONST.LEARNING_RATE_ALPHA * td_error * stats.eligibility
      
      -- Decay eligibility traces for all actions in this tactic
      for actionName, actionStats in pairs(tacticStats) do
          if type(actionStats) == "table" and actionStats.eligibility then
              if actionName ~= action then
                  actionStats.eligibility = actionStats.eligibility * CONST.DISCOUNT_FACTOR_GAMMA * CONST.ELIGIBILITY_LAMBDA
              end
          end
      end

      stats.count += 1
      tacticStats.total += 1
      AI.ActionStats.totalTrials += 1

      -- Adjust tactic confidence based on performance
      if rawReward > 0 then
          AI.State.tacticConfidence = math.min(150, AI.State.tacticConfidence + rawReward * 0.5)
      else
          AI.State.tacticConfidence = math.max(0, AI.State.tacticConfidence + rawReward * 0.5)
      end
      
      -- Update epsilon for exploration-exploitation
      AI.State.epsilon = math.max(CONST.MIN_EPSILON, AI.State.epsilon * CONST.EPSILON_DECAY)
  end
  
  -- Perform experience replay learning
  function AI.Learning.ReplayExperiences()
      local samples = AI.Learning.SampleExperiences(CONST.REPLAY_BATCH_SIZE)
      
      for _, experience in ipairs(samples) do
          local tacticStats = AI.ActionStats[experience.tactic]
          if tacticStats and tacticStats[experience.action] then
              -- Calculate best Q-value for next state
              local nextMaxQ = AI.Learning.GetBestNextQValue(experience.tactic, {
                  ATTACK = true, SPECIAL = true, EVADE = true, FEINT = true, REPOSITION = true
              })
              
              -- Perform Q-learning update
              local stats = tacticStats[experience.action]
              local old_value = stats.value
              local td_error = experience.reward + CONST.DISCOUNT_FACTOR_GAMMA * nextMaxQ - old_value
              stats.value = old_value + CONST.LEARNING_RATE_ALPHA * 0.5 * td_error -- Use half learning rate for replay
          end
      end
  end

  function AI.Learning.GetBestNextQValue(tactic, possibleActions)
      local tacticStats = AI.ActionStats[tactic]
      if not tacticStats then return 0 end

      local maxQ = -math.huge
      local foundAction = false
      for action, available in pairs(possibleActions) do
          if available and tacticStats[action] and tacticStats[action].count > 0 then
              local stats = tacticStats[action]
              local qValue = stats.value / stats.count -- Use average as Q-value
              if qValue > maxQ then
                  maxQ = qValue
                  foundAction = true
              end
          end
      end
      return foundAction and maxQ or 0
  end
  
  -- Extract deep features from game state
  function AI.Learning.ExtractFeatures(state)
      local features = {
          healthDiff = (state.myHealthPercent - state.targetHealthPercent) * AI.FeatureWeights.healthDiff,
          distanceNorm = (1 - state.distance / CONST.AGGRO_RANGE) * AI.FeatureWeights.distanceToTarget,
          energyNorm = (state.energy / CONST.MAX_ENERGY) * AI.FeatureWeights.energyLevel,
          inRangeBinary = state.inRange and 1 or 0,
          targetAttackingBinary = state.isTargetAttacking and 1 or 0,
      }
      
      -- Compute feature score
      local score = 0
      for _, value in pairs(features) do
          score += value
      end
      
      return features, score
  end

  --// --- EVALUATION MODULE ---
  function AI.Evaluation.ChooseTactic(state)
      local now = os.clock()

      -- Highest priority: Glitch exploitation.
      if AI.GlitchScanner.isGlitchDetected and now - AI.GlitchScanner.detectionTime < 5.0 then
          if AI.State.currentTactic ~= "ExploitSeeker" then
              print(`GLITCH DETECTED (${AI.GlitchScanner.glitchType})! Forcing EXPLOITSEEKER tactic.`)
              -- Reinforce the action that *caused* the glitch.
              AI.Learning.Reinforce("ExploitSeeker", AI.State.lastAction, CONST.GLITCH_DETECTION_REWARD, 0) -- No future Q-value needed here.
          end
          return "ExploitSeeker"
      end

      -- Cooldown to prevent rapid tactic switching.
      if now - AI.State.lastTacticChangeTime < CONST.TACTIC_CHANGE_COOLDOWN then
          return AI.State.currentTactic
      end

      -- If confidence is critical, switch to experimental mode.
      if AI.State.tacticConfidence < CONST.EXPLOIT_SEEKER_CONFIDENCE_THRESHOLD and now - AI.State.lastExploitSeekerTriggerTime > CONST.EXPLOIT_SEEKER_COOLDOWN then
          AI.State.lastTacticChangeTime = now
          AI.State.lastExploitSeekerTriggerTime = now
          print("Confidence critical. Switching to EXPLOITSEEKER to find new strategies.")
          return "ExploitSeeker"
      end

      -- If confidence is high, stick with the current tactic for a bit longer.
      if AI.State.tacticConfidence > CONST.TACTIC_CONFIDENCE_THRESHOLD and os.clock() - AI.State.lastTacticChangeTime < CONST.TACTIC_CHANGE_COOLDOWN * 2 then
          return AI.State.currentTactic
      end

      -- Rule-based tactic selection.
      local newTactic
      if state.targetHealthPercent < 0.2 and state.distance < CONST.ATTACK_RANGE * 1.5 then newTactic = "Finisher"
      elseif state.myHealthPercent > 0.75 and state.energy > (CONST.MAX_ENERGY * 0.6) then newTactic = "Aggressive"
      elseif state.myHealthPercent < 0.35 or state.energy < (CONST.MAX_ENERGY * 0.25) then newTactic = "Defensive"
      else newTactic = "BaitAndPunish" end

      if newTactic ~= AI.State.currentTactic then
          AI.State.lastTacticChangeTime = now
          AI.State.tacticConfidence = 100 -- Reset confidence on switch.
          print("Tactic changed to:", newTactic)
      end
      return newTactic
  end

  function AI.Evaluation.FindBestActionByScore(state, tactic)
      local now = os.clock()

      -- Immediate capitalization windows (glitch or punish) - highest priority
      local function isCapitalizeWindow()
          return now < AI.State.glitchComboWindowEndTime or now < AI.State.punishWindowEndTime
      end

      if isCapitalizeWindow() and state.inRange then
          if AI.State.glitchComboWindowEndTime > now then print("Executing Glitch Combo!") end
          if #AI.AvailableMoves > 0 and now - AI.State.lastActionTimes.SPECIAL > CONST.SPECIAL_MOVE_COOLDOWN and state.energy >= CONST.COSTS.SPECIAL then return "SPECIAL" end
          if now - AI.State.lastActionTimes.ATTACK > CONST.ATTACK_COOLDOWN then return "ATTACK" end
      end

      -- Cache possible actions check for performance
      local possibleActions = {}
      local inCombo = now < AI.State.comboStateEndTime
      possibleActions.ATTACK = (now - AI.State.lastActionTimes.ATTACK > (inCombo and CONST.ATTACK_COOLDOWN * 0.5 or CONST.ATTACK_COOLDOWN)) and state.inRange
      possibleActions.SPECIAL = (now - AI.State.lastActionTimes.SPECIAL > CONST.SPECIAL_MOVE_COOLDOWN) and (state.energy >= CONST.COSTS.SPECIAL) and state.inRange and (#AI.AvailableMoves > 0)
      possibleActions.EVADE = (now - AI.State.lastActionTimes.EVADE > CONST.DASH_COOLDOWN) and (state.energy >= CONST.COSTS.DASH)
      possibleActions.FEINT = (now - AI.State.lastActionTimes.FEINT > CONST.FEINT_COOLDOWN) and (state.energy >= CONST.COSTS.FEINT) and state.inRange
      possibleActions.REPOSITION = true

      -- ExploitSeeker uses pure random exploration
      if tactic == "ExploitSeeker" then
          local actionPool = {}
          for action, available in pairs(possibleActions) do
              if available and (action == "ATTACK" or action == "SPECIAL" or action == "EVADE" or action == "FEINT") then
                  table.insert(actionPool, action)
              end
          end
          return #actionPool > 0 and actionPool[math.random(#actionPool)] or "REPOSITION"
      end

      -- Epsilon-greedy exploration: sometimes choose random action
      if math.random() < AI.State.epsilon then
          local actionPool = {}
          for action, available in pairs(possibleActions) do
              if available then table.insert(actionPool, action) end
          end
          if #actionPool > 0 then
              return actionPool[math.random(#actionPool)]
          end
      end

      -- Extract deep features for better decision-making
      local _, featureScore = AI.Learning.ExtractFeatures(state)

      local tacticStats = AI.ActionStats[tactic]
      local actionScores = {}

      for action, available in pairs(possibleActions) do
          if available then
              local stats = tacticStats[action]
              local score
              
              if stats.count == 0 then
                  score = 100 -- High value for untried actions
              else
                  -- Combine Q-value with feature score
                  local qValue = stats.value / stats.count
                  score = qValue + featureScore * 0.1
              end

              -- Apply situational modifiers for tactical awareness
              local modifier = 1.0
              if action == "EVADE" and state.isTargetAttacking then 
                  modifier = 2.0 -- Strong preference for evasion when under attack
              end
              if (tactic == "BaitAndPunish" or tactic == "Defensive") and action == "EVADE" then 
                  modifier *= 1.3 
              end
              if action == "ATTACK" and inCombo then
                  modifier *= 1.4 -- Encourage combo continuations
              end

              table.insert(actionScores, { action = action, score = score * modifier })
          end
      end

      if #actionScores == 0 then return "REPOSITION" end

      -- Sort by score (highest first)
      table.sort(actionScores, function(a, b) return a.score > b.score end)

      return actionScores[1].action
  end

  --// --- ACTION MODULE ---
  function AI.Actions.Execute(action, state)
      local now = os.clock()
      AI.State.lastAction = action

      if not AI.State.targetTorso then return end -- Safety check
      local targetPos = AI.State.targetTorso.Position

      local cost = CONST.COSTS[action:upper()] or 0
      AI.State.currentEnergy -= cost
      
      -- Store current state for experience replay
      AI.State.actionsSinceReplay += 1

      if action == "FEINT" then
          communicateEvent:FireServer({["Goal"]="Feint"})
          AI.State.lastActionTimes.FEINT = now
          AI.State.isFeinting, AI.State.feintStartTime, AI.State.distanceAtFeintStart = true, now, (rootPart.Position - targetPos).Magnitude
      elseif action == "ATTACK" then
          communicateEvent:FireServer({["Mobile"]=true, ["Goal"]="LeftClick"})
          AI.State.lastActionTimes.ATTACK = now
          AI.State.punishWindowEndTime = 0
      elseif action == "SPECIAL" then
          if #AI.AvailableMoves > 0 then
              local moveName = AI.AvailableMoves[math.random(#AI.AvailableMoves)]
              local tool = player.Backpack:FindFirstChild(moveName) or character:FindFirstChild(moveName)
              if tool then
                  communicateEvent:FireServer({["Tool"] = tool, ["Goal"] = "Console Move"})
                  AI.State.lastActionTimes.SPECIAL = now
                  AI.State.punishWindowEndTime = 0
              end
          end
      elseif action == "EVADE" then
          -- Smarter evade direction based on target velocity
          local targetVelocity = AI.State.targetTorso.AssemblyLinearVelocity
          local perpendicular = rootPart.CFrame.RightVector * (math.random() > 0.5 and 1 or -1)
          if targetVelocity.Magnitude > 10 then
              -- Dodge perpendicular to target's movement
              local targetDir = targetVelocity.Unit
              perpendicular = Vector3.new(-targetDir.Z, 0, targetDir.X) -- 90 degree rotation
          end
          communicateEvent:FireServer({["Dash"]=perpendicular.Unit, ["Key"]="Q", ["Goal"]="KeyPress"})
          AI.State.lastActionTimes.EVADE = now
          AI.State.isPerfectDodging, AI.State.dodgeStartTime = true, now
      elseif action == "REPOSITION" then
          if now - AI.State.lastStrafeChange > (0.8 + math.random() * 0.4) then -- Faster strafe changes
              AI.State.strafeDirection *= -1
              AI.State.lastStrafeChange = now
          end

          local currentDist = (rootPart.Position - targetPos).Magnitude
          local desiredRange
          if AI.State.currentTactic == "Defensive" then desiredRange = CONST.KITE_RANGE
          elseif AI.State.currentTactic == "Finisher" or AI.State.currentTactic == "Aggressive" then desiredRange = CONST.ATTACK_RANGE * 0.7
          else desiredRange = CONST.ATTACK_RANGE * 0.9 end

          local moveDirection = (targetPos - ((targetPos - rootPart.Position).Unit * desiredRange) + (rootPart.CFrame.RightVector * 10 * AI.State.strafeDirection))
          humanoid:MoveTo(moveDirection)

          -- Immediate feedback for repositioning
          local distanceError = math.abs(currentDist - desiredRange)
          local reward = math.max(0, 1 - (distanceError / desiredRange)) * 2
          AI.Learning.Reinforce(AI.State.currentTactic, "REPOSITION", reward, 0)
      end
      
      -- Store experience for replay learning
      if state then
          AI.Learning.StoreExperience(state, action, 0, state, AI.State.currentTactic)
      end
      
      -- Perform experience replay periodically
      if AI.State.actionsSinceReplay >= CONST.REPLAY_FREQUENCY then
          AI.Learning.ReplayExperiences()
          AI.State.actionsSinceReplay = 0
      end
  end

  --// ================= HELPER & UPDATE FUNCTIONS ================= //

  local function Cleanup()
      if targetDiedConnection then targetDiedConnection:Disconnect(); targetDiedConnection = nil end
      for _, c in ipairs(connections) do c:Disconnect() end
      connections = {}
      AI.State.combatState = "IDLE"
      AI.State.currentTarget = nil
      print("Project Apex has been shut down for this character.")
  end

  local function GetTargetScore(targetCharacter)
      -- Fast validity checks with early returns
      if not targetCharacter or not targetCharacter.Parent then return 0 end
      local root = targetCharacter:FindFirstChild("HumanoidRootPart")
      if not root then return 0 end
      
      local targetHumanoid = targetCharacter:FindFirstChildOfClass("Humanoid")
      if not targetHumanoid or targetHumanoid.Health <= 0 then return 0 end

      -- Pre-calculate distance once
      local distance = (rootPart.Position - root.Position).Magnitude
      if distance > CONST.AGGRO_RANGE then return 0 end

      -- Optimized scoring with weighted factors
      local healthRatio = targetHumanoid.Health / targetHumanoid.MaxHealth
      local healthScore = (1 - healthRatio) * 2.0 -- Increased weight for low-health targets
      local distanceScore = (1 - (distance / CONST.AGGRO_RANGE)) * 1.5 -- Closer is better
      
      -- Improved velocity penalty with gradual scaling
      local velocityMag = root.AssemblyLinearVelocity.Magnitude
      local velocityScore = velocityMag > 25 and -(velocityMag / 50) or 0.2 -- Bonus for stationary targets

      return healthScore + distanceScore + velocityScore
  end

  local function EvaluateAndSetTarget()
      local bestTarget, highestScore = nil, -math.huge
      for _, p in ipairs(Players:GetPlayers()) do
          if p ~= player and p.Character then
              local score = GetTargetScore(p.Character)
              if score > highestScore then
                  highestScore, bestTarget = score, p.Character
              end
          end
      end

      if bestTarget and bestTarget ~= AI.State.currentTarget then
          if targetDiedConnection then targetDiedConnection:Disconnect() end

          local targetHumanoid = bestTarget:FindFirstChildOfClass("Humanoid")
          AI.State.currentTarget, AI.State.targetHumanoid, AI.State.targetTorso = bestTarget, targetHumanoid, bestTarget:FindFirstChild("Torso") or bestTarget:FindFirstChild("HumanoidRootPart")
          AI.State.lastTargetHealth = targetHumanoid and targetHumanoid.Health or 0

          targetDiedConnection = targetHumanoid.Died:Connect(function()
              AI.Score += 10
              print(`Target eliminated. New Score: {AI.Score}`)
              AI.State.currentTarget = nil
          end)
      end

      if not AI.State.currentTarget or not AI.State.targetHumanoid or AI.State.targetHumanoid.Health <= 0 then
          AI.State.combatState = "IDLE"
          AI.State.currentTarget = nil
      else
          AI.State.combatState = "ENGAGING"
      end
  end

  local function RunGlitchScanner()
      local now = os.clock()
      
      -- Quick exit if glitch is already active
      if AI.GlitchScanner.isGlitchDetected then
          if now - AI.GlitchScanner.detectionTime > 2.0 then
              AI.GlitchScanner.isGlitchDetected = false
          end
          return
      end

      local function triggerGlitch(glitchType)
          AI.GlitchScanner.isGlitchDetected = true
          AI.GlitchScanner.glitchType = glitchType
          AI.GlitchScanner.detectionTime = now
          AI.State.glitchComboWindowEndTime = now + CONST.GLITCH_COMBO_WINDOW_DURATION
          print(`GLITCH DETECTED: {glitchType}`)
      end

      -- 1. Fast velocity spike detection (optimized)
      local currentVelocity = rootPart.AssemblyLinearVelocity
      local currentVelocityMag = currentVelocity.Magnitude
      if currentVelocityMag > CONST.GLITCH_VELOCITY_THRESHOLD and AI.GlitchScanner.lastVelocityMagnitude < CONST.GLITCH_VELOCITY_THRESHOLD then
          triggerGlitch("Velocity Spike")
          AI.GlitchScanner.lastVelocityMagnitude = currentVelocityMag
          return
      end
      AI.GlitchScanner.lastVelocityMagnitude = currentVelocityMag

      -- 2. Humanoid state anomaly detection (optimized with early return)
      local currentState = humanoid:GetState()
      if currentState ~= AI.GlitchScanner.lastHumanoidState then
          if currentState == Enum.HumanoidStateType.StrafingNoPhysics or currentState == Enum.HumanoidStateType.None then
              triggerGlitch("Humanoid State Anomaly")
              AI.GlitchScanner.lastHumanoidState = currentState
              return
          end
          AI.GlitchScanner.lastHumanoidState = currentState
      end

      -- 3. NaN position check (fast)
      local pos = rootPart.Position
      if pos.X ~= pos.X or pos.Y ~= pos.Y or pos.Z ~= pos.Z then
          triggerGlitch("NaN Position")
      end
  end

  local function UpdateAIState(deltaTime)
      local now = os.clock()
      AI.State.currentEnergy = math.min(CONST.MAX_ENERGY, AI.State.currentEnergy + CONST.ENERGY_REGEN_RATE * deltaTime)

      if now - AI.State.lastTargetScanTime > CONST.TARGET_SCAN_INTERVAL then
          EvaluateAndSetTarget()
          AI.State.lastTargetScanTime = now
      end

      if AI.State.currentTarget and AI.State.currentTarget.Parent then
          local activeTool = AI.State.currentTarget:FindFirstChildOfClass("Tool")
          AI.State.isTargetAttacking = activeTool and activeTool.Enabled
      else
          AI.State.isTargetAttacking = false
      end

      RunGlitchScanner() -- Run the scanner every state update.
  end

  local function UpdateLearningAndRewards(now)
      -- Fast exit if no valid target
      if not AI.State.currentTarget or not AI.State.targetHumanoid or not AI.State.targetTorso then return end

      local tactic = AI.State.currentTactic

      -- Feint evaluation (optimized timing check)
      if AI.State.isFeinting and now > AI.State.feintStartTime + CONST.FEINT_DURATION then
          AI.State.isFeinting = false
          local currentDistance = (rootPart.Position - AI.State.targetTorso.Position).Magnitude
          local feintSucceeded = currentDistance > AI.State.distanceAtFeintStart + CONST.FEINT_SUCCESS_DISTANCE
          local reward = feintSucceeded and CONST.REWARD_FEINT_SUCCESS or CONST.PENALTY_FAILED_ACTION
          AI.Learning.Reinforce(tactic, "FEINT", reward, 0)
          if feintSucceeded then
              AI.State.punishWindowEndTime = now + CONST.PUNISH_WINDOW_DURATION
              print("Feint successful! Punish window opened.")
          end
      end

      -- Perfect dodge window timeout
      if AI.State.isPerfectDodging and now > AI.State.dodgeStartTime + CONST.PERFECT_DODGE_WINDOW then
          AI.State.isPerfectDodging = false
      end

      -- Damage dealt evaluation with n-step return consideration
      local currentTargetHealth = AI.State.targetHumanoid.Health
      local damageDealt = AI.State.lastTargetHealth - currentTargetHealth
      if damageDealt > 0 then
          local action = AI.State.lastAction
          local reward
          
          -- Anomaly detection for unexpected damage sources
          if action ~= "ATTACK" and action ~= "SPECIAL" then
              print(`ANOMALY: Damage (${damageDealt}) dealt via ${action}. High reward!`)
              reward = CONST.REWARD_ANOMALOUS_DAMAGE
          else
              reward = damageDealt * CONST.REWARD_DAMAGE_DEALT_MULTIPLIER
              -- Bonus for combo damage
              if now < AI.State.comboStateEndTime then
                  reward *= 1.3
              end
          end

          -- Calculate next state Q-value for temporal difference learning
          local nextStateActions = { ATTACK = true, SPECIAL = true, EVADE = true, FEINT = true, REPOSITION = true }
          local nextMaxQ = AI.Learning.GetBestNextQValue(tactic, nextStateActions)
          AI.Learning.Reinforce(tactic, action, reward, nextMaxQ)

          -- Extend combo window
          AI.State.comboStateEndTime = now + CONST.COMBO_WINDOW_DURATION
          
          -- Update feature weights based on success
          AI.FeatureWeights.tacticSuccess = math.min(2.0, AI.FeatureWeights.tacticSuccess * 1.05)
      else
          -- Decay tactic success weight if no damage
          AI.FeatureWeights.tacticSuccess = math.max(0.5, AI.FeatureWeights.tacticSuccess * 0.99)
      end
      
      AI.State.lastTargetHealth = currentTargetHealth
  end

  local function ExecuteCombatLogic(now)
      if AI.State.combatState ~= "ENGAGING" or AI.State.isFeinting then return end
      if not (AI.State.targetTorso and AI.State.targetTorso.Parent and AI.State.targetHumanoid) then return end

      -- Optimized prediction with caching
      local shouldRecalcPrediction = not AI.State.cachedPredictedPos or (now - AI.State.lastPredictionTime) > 0.1
      if shouldRecalcPrediction then
          local ping = player:GetNetworkPing()
          AI.State.cachedPredictedPos = AI.State.targetTorso.Position + AI.State.targetTorso.AssemblyLinearVelocity * (CONST.BASE_PREDICTION_TIME + ping)
          AI.State.lastPredictionTime = now
      end
      
      -- Fast aim update
      rootPart.CFrame = CFrame.lookAt(rootPart.Position, Vector3.new(AI.State.cachedPredictedPos.X, rootPart.Position.Y, AI.State.cachedPredictedPos.Z))

      -- Pre-calculate distance once
      local distance = (rootPart.Position - AI.State.targetTorso.Position).Magnitude
      
      -- Build current state object for decision making (optimized)
      local state = {
          myHealthPercent = humanoid.Health / humanoid.MaxHealth,
          targetHealthPercent = AI.State.targetHumanoid.Health / AI.State.targetHumanoid.MaxHealth,
          distance = distance,
          energy = AI.State.currentEnergy,
          inRange = distance <= CONST.ATTACK_RANGE,
          isTargetAttacking = AI.State.isTargetAttacking,
      }

      AI.State.currentTactic = AI.Evaluation.ChooseTactic(state)
      local bestAction = AI.Evaluation.FindBestActionByScore(state, AI.State.currentTactic)
      AI.Actions.Execute(bestAction, state)
  end

  --// ================= EVENT CONNECTIONS ================= //
  table.insert(connections, humanoid.HealthChanged:Connect(function(newHealth)
      local damageTaken = AI.State.myHealth - newHealth
      if damageTaken > 0 then
          local penalty = damageTaken * CONST.PENALTY_DAMAGE_TAKEN_MULTIPLIER
          local failedAction = AI.State.lastAction
          if AI.State.isPerfectDodging and now > AI.State.dodgeStartTime then -- Check if dodge was active
              failedAction = "EVADE"
              AI.Learning.Reinforce(AI.State.currentTactic, "EVADE", CONST.PENALTY_FAILED_ACTION, 0)
          else
              AI.Learning.Reinforce(AI.State.currentTactic, failedAction, penalty, 0)
          end
          AI.State.isPerfectDodging = false
      elseif AI.State.isPerfectDodging and AI.State.isTargetAttacking then
          AI.Learning.Reinforce(AI.State.currentTactic, "EVADE", CONST.REWARD_PERFECT_DODGE, 0)
          AI.State.punishWindowEndTime = os.clock() + CONST.PUNISH_WINDOW_DURATION
          print("Perfect Dodge! Opening Punish Window.")
          AI.State.isPerfectDodging = false
      end
      AI.State.myHealth = newHealth
  end))

  table.insert(connections, LogService.MessageOut:Connect(function(message, messageType)
      if messageType == Enum.MessageType.MessageError and not AI.GlitchScanner.isGlitchDetected then
          local now = os.clock()
          AI.GlitchScanner.isGlitchDetected = true
          AI.GlitchScanner.glitchType = "Game Script Error"
          AI.GlitchScanner.detectionTime = now
          AI.State.glitchComboWindowEndTime = now + CONST.GLITCH_COMBO_WINDOW_DURATION
      end
  end))

  table.insert(connections, humanoid.Died:Connect(function()
      AI.Score -= 10
      print(`Project Apex defeated. New Score: {AI.Score}.`)
      Cleanup()
  end))
  table.insert(connections, character.AncestryChanged:Connect(function(_, parent)
      if not parent then Cleanup() end
  end))

  -- The main heartbeat loop that drives the AI.
  table.insert(connections, RunService.Heartbeat:Connect(function(deltaTime)
      local success, err = pcall(function()
          local now = os.clock()
          UpdateAIState(deltaTime)
          UpdateLearningAndRewards(now)
          ExecuteCombatLogic(now)
      end)
      if not success then
          warn("Project Apex encountered an error in its main loop:", err)
      end
  end))
end

--// ================= SCRIPT ENTRY POINT ================= //
-- Connect the initialization function to the CharacterAdded event.
player.CharacterAdded:Connect(initializeCombatSystem)
-- If the character already exists when the script runs, initialize immediately.
if player.Character then
  initializeCombatSystem(player.Character)
end
